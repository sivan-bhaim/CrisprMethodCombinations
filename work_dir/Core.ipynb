{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Core",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ6Ea1G2UmQM"
      },
      "source": [
        "# Improving the prediction of e\u000eciency of CRISPR/Cas9 guides using method combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d6cWeYm6ZDw"
      },
      "source": [
        "General instructions\n",
        "====================\n",
        "\n",
        "The following sections should be run from start to finish, since they form the backend of this notebook:\n",
        "- Prerequisites\n",
        "- Utils\n",
        "- Comparison tools\n",
        "- Architecture\n",
        "\n",
        "The Pipeline section is where you configure and train a the model. Choose between creating a new experiment or loading an existing one.\n",
        "\n",
        "Next follows the Comparisons section. It provides various plots and tables for evaluating the model and comparing it to existing tools.\n",
        "\n",
        "Finally, the Feature importance section produces SHAP charts for the current model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwrkrMApiOHq"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqXzjwGkBJeX"
      },
      "source": [
        "#@title Setup { form-width: \"150px\" }\n",
        "\n",
        "# Some warning and errors will be displayed, however they do not affect the\n",
        "# functionality of the notebook.\n",
        "# After running this cell, the runtime should be restarted.\n",
        "\n",
        "!pip freeze | grep == | sed 's/==/>=/' > constraints.txt\n",
        "\n",
        "!pip install -c constraints.txt spacecutter\n",
        "!pip install -c constraints.txt -U skorch\n",
        "!pip install -c constraints.txt https://github.com/ceshine/shap/archive/master.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx50aC7UiGmj"
      },
      "source": [
        "#@title Constants { form-width: \"150px\" }\n",
        "\n",
        "import os\n",
        "\n",
        "# Files are loaded into the notebook through Google Drive.\n",
        "GDRIVE_PATH = \"/content/drive\"\n",
        "# Remember to adjust this path according to the location of the main work\n",
        "# directory inside the drive.\n",
        "WORK_DIR = os.path.join(GDRIVE_PATH, \"My Drive/some_path/work_dir\")\n",
        "DATA_DIR = os.path.join(WORK_DIR, \"data\")\n",
        "\n",
        "# Remember to adjust these paths to match the files according to their location\n",
        "# inside the main work directory. If the default paths were used, these should\n",
        "# work without modification.\n",
        "CHARI_DATA_PATH = os.path.join(DATA_DIR, \"chari1_features.csv\")\n",
        "GENOME_DATA_PATH = os.path.join(DATA_DIR, \"genome1_features.csv\")\n",
        "XU_DATA_PATH = os.path.join(DATA_DIR, \"xu1_features.csv\")       # Ribosomal\n",
        "NR_XU_DATA_PATH = os.path.join(DATA_DIR, \"xu2_features.csv\")    # Nonribosomal\n",
        "DOENCH_DATA_PATH = os.path.join(DATA_DIR, \"doench1_features.csv\")\n",
        "MIXTURE_DATA_PATH = os.path.join(DATA_DIR, \"mixture\", \"mixture.pkl\")\n",
        "# Only required for performing DeepCRISPR comparisons.\n",
        "# If these are required, follow the instructions in the README.txt inside the\n",
        "# data/deepcrispr directory to generate the necessary output files.\n",
        "DEEPCRISPR_PATH = os.path.join(DATA_DIR, \"deepcrispr\")\n",
        "\n",
        "# Common training, validation and testing dataset sizes\n",
        "SIZES_6K = (6000, 1000, 1000)\n",
        "SIZES_12K = (12000, 1500, 1500)\n",
        "\n",
        "# The number of features in the feature representation. Remember to change this\n",
        "# if the feature representation was changed from the original.\n",
        "NUM_FEATURES = 18\n",
        "\n",
        "os.environ[\"WORK_DIR\"] = WORK_DIR\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9r9-wks-XG6h"
      },
      "source": [
        "#@title Features { form-width: \"150px\" }\n",
        "\n",
        "FEATURES = [\n",
        "    \"CHOPCHOP: GC content\",                     # 0\n",
        "    \"CHOPCHOP: self\\ncomplementarity\",          # 1\n",
        "    \"CHOPCHOP: Xu 2015\",                        # 2\n",
        "    \"CHOPCHOP: Doench 2014\",                    # 3\n",
        "    \"CHOPCHOP: Moreno-\\nMateos 2015\",           # 4\n",
        "    \"CHOPCHOP: G20\",                            # 5\n",
        "    \"FlashFry: Doench 2014\",                    # 6\n",
        "    \"FlashFry: Moreno-\\nMateos 2015\",           # 7\n",
        "    \"mm10db: AT content\",                       # 8\n",
        "    \"mm10db: multiple\\nmatches\",                # 9\n",
        "    \"mm10db: secondary\\nstrcuture or energy\",   # 10\n",
        "    \"mm10db: reverse primer\",                   # 11\n",
        "    \"mm10db: TTTT\",                             # 12\n",
        "    \"mm10db: off-target\",                       # 13\n",
        "    \"mm10db: accepted\",                         # 14\n",
        "    \"PhytoCRISP-Ex\",                            # 15\n",
        "    \"sgRNA Scorer 2.0\",                         # 16\n",
        "    \"SSC\",                                      # 17\n",
        "]\n",
        "\n",
        "COL_TO_SCORE_TOOL = {i: FEATURES[i] for i in [2, 3, 4, 6, 7, 16, 17]}\n",
        "COL_TO_DECISION_TOOL = {i: FEATURES[i] for i in [5, 14, 15]}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKtxvLr-igWf"
      },
      "source": [
        "#@title Mount Google Drive { form-width: \"150px\" }\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(GDRIVE_PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkl2fb7EiS1P"
      },
      "source": [
        "#@title Imports { form-width: \"150px\" }\n",
        "\n",
        "import gc\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import copy\n",
        "import shap\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as clr\n",
        "\n",
        "from random import randint\n",
        "from sklearn import metrics\n",
        "from datetime import datetime\n",
        "from itertools import product\n",
        "from IPython.display import display, HTML\n",
        "from spacecutter.losses import CumulativeLinkLoss\n",
        "from spacecutter.models import OrdinalLogisticModel\n",
        "from spacecutter.callbacks import AscensionCallback\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.autograd import Variable\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSm1oNVYmFU8"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsRfbtfajIXU"
      },
      "source": [
        "## Data utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rNHN2ozjv2W"
      },
      "source": [
        "#@title DataPoint class { form-width: \"150px\" }\n",
        "\n",
        "class DataPoint(object):\n",
        "    \"\"\"Represents a single datapoint.\n",
        "\n",
        "    Attributes:\n",
        "        target: A string of the target sequence.\n",
        "        label: The GenomeCRISPR effect label of the target.\n",
        "        features: The feature representation of the target.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, target, label, features):\n",
        "        self.target = target\n",
        "        self.label = label\n",
        "        self.features = features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yjV-BaSiWDU"
      },
      "source": [
        "#@title Read data { form-width: \"150px\" }\n",
        "\n",
        "def from_csv_line(line):\n",
        "    \"\"\"Extracts a list of values from the string of a CSV line\"\"\"\n",
        "    return line.strip().split(',')\n",
        "\n",
        "def get_data(path, req_features=[]):\n",
        "    \"\"\"Reads data from a dataset CSV file into a list of DataPoints.\n",
        "\n",
        "    Args:\n",
        "        path: The full path of the dataset.\n",
        "        req_features: A list of indices of features to include in the output.\n",
        "            If the list is empty (which is the default) - all features are\n",
        "            included.\n",
        "    \n",
        "    Returns:\n",
        "        A list of DataPoint instances, each matching a single datapoint in the\n",
        "        dataset.\n",
        "    \"\"\"\n",
        "    datapoints = []\n",
        "    with open(path, 'r') as fd:\n",
        "        for line in fd:\n",
        "            values = from_csv_line(line)\n",
        "            target = values[0]\n",
        "            label  = int(values[1])\n",
        "            features = [float(score) for score in values[2:]]\n",
        "            if req_features:\n",
        "                features = [features[i] for i in req_features]\n",
        "            datapoints.append(DataPoint(target, label, features))\n",
        "    return datapoints\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYa6Q7ilkuCO"
      },
      "source": [
        "#@title Parse data { form-width: \"150px\" }\n",
        "\n",
        "\"\"\"Utilities for extracting information from a list of DataPoint instances.\"\"\"\n",
        "\n",
        "def get_targets(datapoints):\n",
        "    return [dp.target for dp in datapoints]\n",
        "\n",
        "def get_labels(datapoints):\n",
        "    return np.array(\n",
        "        [dp.label for dp in datapoints], dtype=np.float32\n",
        "    ).reshape(-1, 1)\n",
        "\n",
        "def get_features(datapoints, num_features):\n",
        "    return np.array(\n",
        "        [dp.features for dp in datapoints], dtype=np.float32\n",
        "    ).reshape(-1, num_features)\n",
        "\n",
        "def labels_to_indices(labels):\n",
        "    \"\"\"Shifts the values of the labels such that the lowest one is 0.\"\"\"\n",
        "    min_label = min(labels)\n",
        "    return [label - min_label for label in labels]\n",
        "\n",
        "def get_labels_as_indices(datapoints):\n",
        "    \"\"\"Extracts the labels and shifts them such that the lowest one is 0.\"\"\"\n",
        "    labels = [dp.label for dp in datapoints]\n",
        "    label_indices = labels_to_indices(labels)\n",
        "    return np.array(label_indices, dtype=np.int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgQagCHkj-_f"
      },
      "source": [
        "#@title DeepCRISPR configuration { form-width: \"150px\" }\n",
        "\n",
        "# Versions\n",
        "REGRESSIONS = \"regression\"\n",
        "CLASSIFICATION = \"classification\"\n",
        "# Available variants\n",
        "VARIANTS = {\n",
        "    REGRESSIONS: [\"seq_only\", \"epigenetics\"],\n",
        "    CLASSIFICATION: [\"epigenetics\"],\n",
        "}\n",
        "\n",
        "def get_deepcrispr_path(version, variant, data_name):\n",
        "    \"\"\"Returns the path of the desired DeepCRISPR output.\n",
        "    \n",
        "    Args:\n",
        "        version: Either REGRESSION or CLASSIFICATION.\n",
        "        variant: One of the variants listed in VARIANTS for the chosen version.\n",
        "        data_name: The name of the dataset required. One of: \"doench\",\n",
        "            \"mixture\", \"xu\", \"nr_xu\".\n",
        "    \"\"\"\n",
        "    return os.path.join(DEEPCRISPR_PATH, version, variant,\n",
        "                        f\"{data_name}_results.pkl\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwm6ffMPJ-uU"
      },
      "source": [
        "#@title Mixture dataset { form-width: \"150px\" }\n",
        "\n",
        "# If the feature representation is different from the original one presented in\n",
        "# the paper, the Mixture.pkl needs to be updated to include the new\n",
        "# representations.\n",
        "\n",
        "def get_mixture_targets(data_dir):\n",
        "    targets_path = os.path.join(data_dir, \"mixture\", \"mixture_targets.txt\")\n",
        "    with open(targets_path, 'r') as mixture_targets:\n",
        "        return [target.strip() for target in mixture_targets.readlines()]\n",
        "\n",
        "def get_datapoints(dataset, targets):\n",
        "    out = []\n",
        "    for dp in dataset:\n",
        "        if not dp.target in targets: continue\n",
        "        out.append(dp)\n",
        "        targets.remove(dp.target)\n",
        "    return out\n",
        "\n",
        "def get_mixture_dataset(datasets):\n",
        "    targets = get_mixture_targets(DATA_DIR)\n",
        "    targets_set = set(targets)\n",
        "    mixture = []\n",
        "    for dataset in datasets:\n",
        "        mixture += get_datapoints(dataset, targets_set)\n",
        "\n",
        "    ordered_mixture = []\n",
        "    for target in targets:\n",
        "        for dp in mixture:\n",
        "            if dp.target != target: continue\n",
        "            ordered_mixture.append(dp)\n",
        "            break\n",
        "    return ordered_mixture\n",
        "\n",
        "def create_mixture_dataset(chari_data, xu_data, doench_data, genome_data):\n",
        "    datasets = [chari_data, xu_data, doench_data, genome_data]\n",
        "    mixture_dataset = get_mixture_dataset(datasets)\n",
        "    with open(MIXTURE_DATA_PATH, 'wb') as fd:\n",
        "            pickle.dump(mixture_dataset, fd)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShlPG-Fnf7em"
      },
      "source": [
        "## Progress tracking utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVKLiBKWmH9E"
      },
      "source": [
        "#@title Training progress { form-width: \"150px\" }\n",
        "\n",
        "def to_mins_secs(time_in_secs):\n",
        "    \"\"\"Converts a time preiod in seconds to a minutes and seconds string\"\"\"\n",
        "    mins = int(time_in_secs) // 60\n",
        "    secs = int(time_in_secs) % 60\n",
        "    return f\"{mins}m {secs}s\"\n",
        "\n",
        "\n",
        "def time_estimate(start_time, completed_fraction):\n",
        "    \"\"\"Computes the elapsed time and the remaining time for the task.\n",
        "    \n",
        "    Args:\n",
        "        start_time: A number represting the start time of the task.\n",
        "        completed_fraction: The fraction of the task already completed.\n",
        "    \n",
        "    Returns:\n",
        "        A string with the elapsed and remaining time.\n",
        "    \"\"\"\n",
        "    now = time.time()\n",
        "    elapsed = now - start_time\n",
        "    total_estiamte = elapsed / completed_fraction\n",
        "    remainig = total_estiamte - elapsed\n",
        "    return f\"{to_mins_secs(elapsed)}\\t({to_mins_secs(remainig)} remaining)\"\n",
        "\n",
        "\n",
        "def report_progress(experiment, start_time, epoch, epochs,\n",
        "                    tot_loss, cur_loss, print_every):\n",
        "    \"\"\"Prints a training progress report string.\n",
        "\n",
        "    Args:\n",
        "        experiment: An Experiment instance to produce the report for.\n",
        "        start_time: The start time of the training.\n",
        "        epoch: The number of the latest epoch completed.\n",
        "        epochs: The total number of epochs in the training session.\n",
        "        tot_loss: The toal loss since the last report.\n",
        "        cur_loss: The loss incurred in the latest epoch.\n",
        "        print_every: The number of epochs between reports.\n",
        "    \"\"\"\n",
        "    avg_loss = tot_loss / print_every\n",
        "\n",
        "    experiment.training.update_progress(\n",
        "        experiment.model.epoch_counter, cur_loss)\n",
        "    validation_loss, test_loss = experiment.test()\n",
        "\n",
        "    elapsed_time = time_estimate(start_time, epoch/epochs)\n",
        "    progress = (epoch/epochs)*100\n",
        "    progress_string = f\"({epoch} {progress}%)\".ljust(15)\n",
        "    print(f\"{elapsed_time}\\t{progress_string} {avg_loss:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l32yF-8L2hWu"
      },
      "source": [
        "#@title DataFrame utils { form-width: \"150px\" }\n",
        "\n",
        "TOOL_COL = \"Tool\"\n",
        "\n",
        "\n",
        "def pretty_print(df):\n",
        "    \"\"\"Prints a DataFrame as a table.\"\"\"\n",
        "    return display(HTML(df.to_html(justify=\"left\").replace(\"\\\\n\",\"<br>\")))\n",
        "\n",
        "\n",
        "def _create_report(stats, tools, format_str=None):\n",
        "    \"\"\"Produces a tool-comparison report DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        stats: A dictionary mapping dataset name to a list of the results of all\n",
        "            the tools, ordered to match the ordering of tools.\n",
        "        tools: A list of the names of the compared tools.\n",
        "        format_str: A formatting string used to format the results. If not\n",
        "            provided, no formatting is applied.\n",
        "    \n",
        "    Returns:\n",
        "        A dataframe where in the first column are the names of the tools, and\n",
        "        each of the following columns corresponds to one of the datasets, and\n",
        "        lists the results of the tools for that dataset.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({TOOL_COL: tools})\n",
        "    for data_name, results in stats.items():\n",
        "        if format_str:\n",
        "            results = [format_str % r for r in results]\n",
        "        df = pd.concat([df, pd.DataFrame({data_name: results})], axis=1)\n",
        "    return df\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKoD7-GgITeH"
      },
      "source": [
        "#@title Plot losses { form-width: \"150px\" }\n",
        "\n",
        "def to_colour(c1, c2, c3):\n",
        "    \"\"\"Converts an RGB in 256-base to a 0-1 colour tuple.\"\"\"\n",
        "    return (c1/255.0, c2/255.0, c3/255.0)\n",
        "\n",
        "\n",
        "def prep_plot():\n",
        "    plt.clf()\n",
        "    plt.style.use('seaborn-colorblind')\n",
        "\n",
        "\n",
        "def plot_losses(data_dict, ylabel=\"Loss\", save_name=\"\"):\n",
        "    \"\"\"Plots loss-per-epoch\n",
        "\n",
        "    Args:\n",
        "        data_dict: A dictionary mapping dataset name to a corresponding Data\n",
        "            instance.\n",
        "        ylable: The title of the y axis (\"Loss\" by default).\n",
        "        save_name: If provided, the plot is saved under this name in the\n",
        "            WORK_DIR.\n",
        "    \"\"\"\n",
        "    prep_plot()\n",
        "    fig = plt.figure()\n",
        "    for name, data in data_dict.items():\n",
        "        plt.plot(data.epochs, data.losses, label=f\"{name} dataset\",\n",
        "                 marker='o', alpha=0.5)\n",
        "    plt.legend(loc='best')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()\n",
        "\n",
        "    if save_name:\n",
        "        save_path = os.path.join(WORK_DIR, save_name)\n",
        "        fig.savefig(save_path, bbox_inches=\"tight\", dpi=100)\n",
        "\n",
        "\n",
        "def plot_standard_losses(experiment, train=True, validation=True, test=False):\n",
        "    \"\"\"Plots loss-per-epoch for the training, validation and tes sets.\n",
        "\n",
        "    Args:\n",
        "        experiment: The Experiment instance to plot for.\n",
        "        train: A boolean indicating whether to plot for the training set.\n",
        "        validation: A boolean indicating whether to plot for the validation set.\n",
        "        test: A boolean indicating whether to plot for the test set.\n",
        "    \"\"\"\n",
        "    datasets = {}\n",
        "    if train: datasets[\"Training\"] = experiment.training\n",
        "    if validation: datasets[\"Validation\"] = experiment.validation\n",
        "    if test: datasets[\"Test\"] = experiment.testing\n",
        "    plot_losses(datasets)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tC_usp4JZnH"
      },
      "source": [
        "## Feature importance utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALdpVlOUKMNF"
      },
      "source": [
        "#@title Colours { form-width: \"150px\" }\n",
        "\n",
        "SHAP_CMAP = clr.ListedColormap(sns.color_palette(\"RdYlBu\", 256))\n",
        "SHAP_COLOUR = sns.color_palette(\"colorblind\", 10)[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k1TcdZeJfHC"
      },
      "source": [
        "#@title Summary plot { form-width: \"150px\" }\n",
        "\n",
        "\"\"\"Adapted from the shap library by CeShine Lee.\n",
        "https://github.com/ceshine/shap\n",
        "\n",
        "Main changes: the addition of the cmap option, and removing the feature names\n",
        "from  the bar plot.\n",
        "\"\"\"\n",
        "\n",
        "from scipy.stats import gaussian_kde\n",
        "from shap.plots import labels\n",
        "from shap.plots import colors\n",
        "\n",
        "pl = plt\n",
        "\n",
        "def summary_plot(shap_values, features=None, feature_names=None,\n",
        "                 max_display=None, plot_type=\"dot\", color=None,\n",
        "                 axis_color=\"#333333\", title=None, alpha=1, show=True,\n",
        "                 sort=True, color_bar=True, auto_size_plot=True,\n",
        "                 layered_violin_max_num_bins=20, class_names=None,\n",
        "                 cmap=colors.red_blue):\n",
        "    \"\"\"Create a SHAP summary plot, colored by feature values when they are provided.\n",
        "    Parameters\n",
        "    ----------\n",
        "    shap_values : numpy.array\n",
        "        Matrix of SHAP values (# samples x # features)\n",
        "    features : numpy.array or pandas.DataFrame or list\n",
        "        Matrix of feature values (# samples x # features) or a feature_names list as shorthand\n",
        "    feature_names : list\n",
        "        Names of the features (length # features)\n",
        "    max_display : int\n",
        "        How many top features to include in the plot (default is 20, or 7 for interaction plots)\n",
        "    plot_type : \"dot\" (default) or \"violin\"\n",
        "        What type of summary plot to produce\n",
        "    cmap : matplotlib.colors.Colormap\n",
        "        A colourmap for the colour bar.\n",
        "    \"\"\"\n",
        "\n",
        "    fig = pl.figure()\n",
        "\n",
        "    multi_class = False\n",
        "    if isinstance(shap_values, list):\n",
        "        multi_class = True\n",
        "        plot_type = \"bar\" # only type supported for now\n",
        "    else:\n",
        "        assert len(shap_values.shape) != 1, \"Summary plots need a matrix of shap_values, not a vector.\"\n",
        "\n",
        "    # default color:\n",
        "    if color is None:\n",
        "        color = \"coolwarm\" if plot_type == 'layered_violin' else \"#1E88E5\" #\"#ff0052\"\n",
        "\n",
        "    # convert from a DataFrame or other types\n",
        "    if str(type(features)) == \"<class 'pandas.core.frame.DataFrame'>\":\n",
        "        if feature_names is None:\n",
        "            feature_names = features.columns\n",
        "        features = features.values\n",
        "    elif isinstance(features, list):\n",
        "        if feature_names is None:\n",
        "            feature_names = features\n",
        "        features = None\n",
        "    elif (features is not None) and len(features.shape) == 1 and feature_names is None:\n",
        "        feature_names = features\n",
        "        features = None\n",
        "\n",
        "    num_features = (shap_values[0].shape[1] if multi_class else shap_values.shape[1])\n",
        "\n",
        "    if feature_names is None:\n",
        "        feature_names = np.array([labels['FEATURE'] % str(i) for i in range(num_features)])\n",
        "\n",
        "    # plotting SHAP interaction values\n",
        "    if not multi_class and len(shap_values.shape) == 3:\n",
        "        if max_display is None:\n",
        "            max_display = 7\n",
        "        else:\n",
        "            max_display = min(len(feature_names), max_display)\n",
        "\n",
        "        sort_inds = np.argsort(-np.abs(shap_values.sum(1)).sum(0))\n",
        "\n",
        "        # get plotting limits\n",
        "        delta = 1.0 / (shap_values.shape[1] ** 2)\n",
        "        slow = np.nanpercentile(shap_values, delta)\n",
        "        shigh = np.nanpercentile(shap_values, 100 - delta)\n",
        "        v = max(abs(slow), abs(shigh))\n",
        "        slow = -v\n",
        "        shigh = v\n",
        "\n",
        "        pl.figure(figsize=(1.5 * max_display + 1, 0.8 * max_display + 1))\n",
        "        pl.subplot(1, max_display, 1)\n",
        "        proj_shap_values = shap_values[:, sort_inds[0], sort_inds]\n",
        "        proj_shap_values[:, 1:] *= 2  # because off diag effects are split in half\n",
        "        summary_plot(\n",
        "            proj_shap_values, features[:, sort_inds] if features is not None else None,\n",
        "            feature_names=feature_names[sort_inds],\n",
        "            sort=False, show=False, color_bar=False,\n",
        "            auto_size_plot=False,\n",
        "            max_display=max_display\n",
        "        )\n",
        "        pl.xlim((slow, shigh))\n",
        "        pl.xlabel(\"\")\n",
        "        title_length_limit = 11\n",
        "        pl.title(shorten_text(feature_names[sort_inds[0]], title_length_limit))\n",
        "        for i in range(1, min(len(sort_inds), max_display)):\n",
        "            ind = sort_inds[i]\n",
        "            pl.subplot(1, max_display, i + 1)\n",
        "            proj_shap_values = shap_values[:, ind, sort_inds]\n",
        "            proj_shap_values *= 2\n",
        "            proj_shap_values[:, i] /= 2  # because only off diag effects are split in half\n",
        "            summary_plot(\n",
        "                proj_shap_values, features[:, sort_inds] if features is not None else None,\n",
        "                sort=False,\n",
        "                feature_names=[\"\" for i in range(len(feature_names))],\n",
        "                show=False,\n",
        "                color_bar=False,\n",
        "                auto_size_plot=False,\n",
        "                max_display=max_display\n",
        "            )\n",
        "            pl.xlim((slow, shigh))\n",
        "            pl.xlabel(\"\")\n",
        "            if i == min(len(sort_inds), max_display) // 2:\n",
        "                pl.xlabel(labels['INTERACTION_VALUE'])\n",
        "            pl.title(shorten_text(feature_names[ind], title_length_limit))\n",
        "        pl.tight_layout(pad=0, w_pad=0, h_pad=0.0)\n",
        "        pl.subplots_adjust(hspace=0, wspace=0.1)\n",
        "        if show:\n",
        "            pl.show()\n",
        "        return\n",
        "\n",
        "    if max_display is None:\n",
        "        max_display = 20\n",
        "\n",
        "    if sort:\n",
        "        # order features by the sum of their effect magnitudes\n",
        "        if multi_class:\n",
        "            feature_order = np.argsort(np.sum(np.mean(np.abs(shap_values), axis=0), axis=0))\n",
        "        else:\n",
        "            feature_order = np.argsort(np.sum(np.abs(shap_values), axis=0))\n",
        "        feature_order = feature_order[-min(max_display, len(feature_order)):]\n",
        "    else:\n",
        "        feature_order = np.flip(np.arange(min(max_display, num_features)), 0)\n",
        "\n",
        "    row_height = 0.4\n",
        "    if auto_size_plot:\n",
        "        pl.gcf().set_size_inches(8, len(feature_order) * row_height + 5.5)\n",
        "    pl.axvline(x=0, color=\"#999999\", zorder=-1)\n",
        "\n",
        "    if plot_type == \"dot\":\n",
        "        for pos, i in enumerate(feature_order):\n",
        "            pl.axhline(y=pos, color=\"#cccccc\", lw=0.5, dashes=(1, 5), zorder=-1)\n",
        "            shaps = shap_values[:, i]\n",
        "            values = None if features is None else features[:, i]\n",
        "            inds = np.arange(len(shaps))\n",
        "            np.random.shuffle(inds)\n",
        "            if values is not None:\n",
        "                values = values[inds]\n",
        "            shaps = shaps[inds]\n",
        "            colored_feature = True\n",
        "            try:\n",
        "                values = np.array(values, dtype=np.float64)  # make sure this can be numeric\n",
        "            except:\n",
        "                colored_feature = False\n",
        "            N = len(shaps)\n",
        "            # hspacing = (np.max(shaps) - np.min(shaps)) / 200\n",
        "            # curr_bin = []\n",
        "            nbins = 100\n",
        "            quant = np.round(nbins * (shaps - np.min(shaps)) / (np.max(shaps) - np.min(shaps) + 1e-8))\n",
        "            inds = np.argsort(quant + np.random.randn(N) * 1e-6)\n",
        "            layer = 0\n",
        "            last_bin = -1\n",
        "            ys = np.zeros(N)\n",
        "            for ind in inds:\n",
        "                if quant[ind] != last_bin:\n",
        "                    layer = 0\n",
        "                ys[ind] = np.ceil(layer / 2) * ((layer % 2) * 2 - 1)\n",
        "                layer += 1\n",
        "                last_bin = quant[ind]\n",
        "            ys *= 0.9 * (row_height / np.max(ys + 1))\n",
        "\n",
        "            if features is not None and colored_feature:\n",
        "                # trim the color range, but prevent the color range from collapsing\n",
        "                vmin = np.nanpercentile(values, 5)\n",
        "                vmax = np.nanpercentile(values, 95)\n",
        "                if vmin == vmax:\n",
        "                    vmin = np.nanpercentile(values, 1)\n",
        "                    vmax = np.nanpercentile(values, 99)\n",
        "                    if vmin == vmax:\n",
        "                        vmin = np.min(values)\n",
        "                        vmax = np.max(values)\n",
        "\n",
        "                assert features.shape[0] == len(shaps), \"Feature and SHAP matrices must have the same number of rows!\"\n",
        "                nan_mask = np.isnan(values)\n",
        "                pl.scatter(shaps[nan_mask], pos + ys[nan_mask], color=\"#777777\", vmin=vmin,\n",
        "                           vmax=vmax, s=16, alpha=alpha, linewidth=0,\n",
        "                           zorder=3, rasterized=len(shaps) > 500)\n",
        "                pl.scatter(shaps[np.invert(nan_mask)], pos + ys[np.invert(nan_mask)],\n",
        "                           cmap=pl.get_cmap(cmap), vmin=vmin, vmax=vmax, s=16,\n",
        "                           c=values[np.invert(nan_mask)], alpha=alpha, linewidth=0,\n",
        "                           zorder=3, rasterized=len(shaps) > 500)\n",
        "            else:\n",
        "\n",
        "                pl.scatter(shaps, pos + ys, s=16, alpha=alpha, linewidth=0, zorder=3,\n",
        "                           color=color if colored_feature else \"#777777\", rasterized=len(shaps) > 500)\n",
        "\n",
        "    elif plot_type == \"violin\":\n",
        "        for pos, i in enumerate(feature_order):\n",
        "            pl.axhline(y=pos, color=\"#cccccc\", lw=0.5, dashes=(1, 5), zorder=-1)\n",
        "\n",
        "        if features is not None:\n",
        "            global_low = np.nanpercentile(shap_values[:, :len(feature_names)].flatten(), 1)\n",
        "            global_high = np.nanpercentile(shap_values[:, :len(feature_names)].flatten(), 99)\n",
        "            for pos, i in enumerate(feature_order):\n",
        "                shaps = shap_values[:, i]\n",
        "                shap_min, shap_max = np.min(shaps), np.max(shaps)\n",
        "                rng = shap_max - shap_min\n",
        "                xs = np.linspace(np.min(shaps) - rng * 0.2, np.max(shaps) + rng * 0.2, 100)\n",
        "                if np.std(shaps) < (global_high - global_low) / 100:\n",
        "                    ds = gaussian_kde(shaps + np.random.randn(len(shaps)) * (global_high - global_low) / 100)(xs)\n",
        "                else:\n",
        "                    ds = gaussian_kde(shaps)(xs)\n",
        "                ds /= np.max(ds) * 3\n",
        "\n",
        "                values = features[:, i]\n",
        "                window_size = max(10, len(values) // 20)\n",
        "                smooth_values = np.zeros(len(xs) - 1)\n",
        "                sort_inds = np.argsort(shaps)\n",
        "                trailing_pos = 0\n",
        "                leading_pos = 0\n",
        "                running_sum = 0\n",
        "                back_fill = 0\n",
        "                for j in range(len(xs) - 1):\n",
        "\n",
        "                    while leading_pos < len(shaps) and xs[j] >= shaps[sort_inds[leading_pos]]:\n",
        "                        running_sum += values[sort_inds[leading_pos]]\n",
        "                        leading_pos += 1\n",
        "                        if leading_pos - trailing_pos > 20:\n",
        "                            running_sum -= values[sort_inds[trailing_pos]]\n",
        "                            trailing_pos += 1\n",
        "                    if leading_pos - trailing_pos > 0:\n",
        "                        smooth_values[j] = running_sum / (leading_pos - trailing_pos)\n",
        "                        for k in range(back_fill):\n",
        "                            smooth_values[j - k - 1] = smooth_values[j]\n",
        "                    else:\n",
        "                        back_fill += 1\n",
        "\n",
        "                vmin = np.nanpercentile(values, 5)\n",
        "                vmax = np.nanpercentile(values, 95)\n",
        "                if vmin == vmax:\n",
        "                    vmin = np.nanpercentile(values, 1)\n",
        "                    vmax = np.nanpercentile(values, 99)\n",
        "                    if vmin == vmax:\n",
        "                        vmin = np.min(values)\n",
        "                        vmax = np.max(values)\n",
        "                pl.scatter(shaps, np.ones(shap_values.shape[0]) * pos, s=9, cmap=colors.red_blue_solid, vmin=vmin, vmax=vmax,\n",
        "                           c=values, alpha=alpha, linewidth=0, zorder=1)\n",
        "                # smooth_values -= nxp.nanpercentile(smooth_values, 5)\n",
        "                # smooth_values /= np.nanpercentile(smooth_values, 95)\n",
        "                smooth_values -= vmin\n",
        "                if vmax - vmin > 0:\n",
        "                    smooth_values /= vmax - vmin\n",
        "                for i in range(len(xs) - 1):\n",
        "                    if ds[i] > 0.05 or ds[i + 1] > 0.05:\n",
        "                        pl.fill_between([xs[i], xs[i + 1]], [pos + ds[i], pos + ds[i + 1]],\n",
        "                                        [pos - ds[i], pos - ds[i + 1]], color=colors.red_blue_solid(smooth_values[i]),\n",
        "                                        zorder=2)\n",
        "\n",
        "        else:\n",
        "            parts = pl.violinplot(shap_values[:, feature_order], range(len(feature_order)), points=200, vert=False,\n",
        "                                  widths=0.7,\n",
        "                                  showmeans=False, showextrema=False, showmedians=False)\n",
        "\n",
        "            for pc in parts['bodies']:\n",
        "                pc.set_facecolor(color)\n",
        "                pc.set_edgecolor('none')\n",
        "                pc.set_alpha(alpha)\n",
        "\n",
        "    elif plot_type == \"layered_violin\":  # courtesy of @kodonnell\n",
        "        num_x_points = 200\n",
        "        bins = np.linspace(0, features.shape[0], layered_violin_max_num_bins + 1).round(0).astype(\n",
        "            'int')  # the indices of the feature data corresponding to each bin\n",
        "        shap_min, shap_max = np.min(shap_values), np.max(shap_values)\n",
        "        x_points = np.linspace(shap_min, shap_max, num_x_points)\n",
        "\n",
        "        # loop through each feature and plot:\n",
        "        for pos, ind in enumerate(feature_order):\n",
        "            # decide how to handle: if #unique < layered_violin_max_num_bins then split by unique value, otherwise use bins/percentiles.\n",
        "            # to keep simpler code, in the case of uniques, we just adjust the bins to align with the unique counts.\n",
        "            feature = features[:, ind]\n",
        "            unique, counts = np.unique(feature, return_counts=True)\n",
        "            if unique.shape[0] <= layered_violin_max_num_bins:\n",
        "                order = np.argsort(unique)\n",
        "                thesebins = np.cumsum(counts[order])\n",
        "                thesebins = np.insert(thesebins, 0, 0)\n",
        "            else:\n",
        "                thesebins = bins\n",
        "            nbins = thesebins.shape[0] - 1\n",
        "            # order the feature data so we can apply percentiling\n",
        "            order = np.argsort(feature)\n",
        "            # x axis is located at y0 = pos, with pos being there for offset\n",
        "            y0 = np.ones(num_x_points) * pos\n",
        "            # calculate kdes:\n",
        "            ys = np.zeros((nbins, num_x_points))\n",
        "            for i in range(nbins):\n",
        "                # get shap values in this bin:\n",
        "                shaps = shap_values[order[thesebins[i]:thesebins[i + 1]], ind]\n",
        "                # if there's only one element, then we can't\n",
        "                if shaps.shape[0] == 1:\n",
        "                    warnings.warn(\n",
        "                        \"not enough data in bin #%d for feature %s, so it'll be ignored. Try increasing the number of records to plot.\"\n",
        "                        % (i, feature_names[ind]))\n",
        "                    # to ignore it, just set it to the previous y-values (so the area between them will be zero). Not ys is already 0, so there's\n",
        "                    # nothing to do if i == 0\n",
        "                    if i > 0:\n",
        "                        ys[i, :] = ys[i - 1, :]\n",
        "                    continue\n",
        "                # save kde of them: note that we add a tiny bit of gaussian noise to avoid singular matrix errors\n",
        "                ys[i, :] = gaussian_kde(shaps + np.random.normal(loc=0, scale=0.001, size=shaps.shape[0]))(x_points)\n",
        "                # scale it up so that the 'size' of each y represents the size of the bin. For continuous data this will\n",
        "                # do nothing, but when we've gone with the unqique option, this will matter - e.g. if 99% are male and 1%\n",
        "                # female, we want the 1% to appear a lot smaller.\n",
        "                size = thesebins[i + 1] - thesebins[i]\n",
        "                bin_size_if_even = features.shape[0] / nbins\n",
        "                relative_bin_size = size / bin_size_if_even\n",
        "                ys[i, :] *= relative_bin_size\n",
        "            # now plot 'em. We don't plot the individual strips, as this can leave whitespace between them.\n",
        "            # instead, we plot the full kde, then remove outer strip and plot over it, etc., to ensure no\n",
        "            # whitespace\n",
        "            ys = np.cumsum(ys, axis=0)\n",
        "            width = 0.8\n",
        "            scale = ys.max() * 2 / width  # 2 is here as we plot both sides of x axis\n",
        "            for i in range(nbins - 1, -1, -1):\n",
        "                y = ys[i, :] / scale\n",
        "                c = pl.get_cmap(color)(i / (\n",
        "                        nbins - 1)) if color in pl.cm.datad else color  # if color is a cmap, use it, otherwise use a color\n",
        "                pl.fill_between(x_points, pos - y, pos + y, facecolor=c)\n",
        "        pl.xlim(shap_min, shap_max)\n",
        "\n",
        "    elif not multi_class and plot_type == \"bar\":\n",
        "        feature_inds = feature_order[:max_display]\n",
        "        y_pos = np.arange(len(feature_inds))\n",
        "        global_shap_values = np.abs(shap_values).mean(0)\n",
        "        pl.barh(y_pos, global_shap_values[feature_inds], 0.7, align='center', color=color)\n",
        "        pl.yticks(y_pos, fontsize=13)\n",
        "        pl.gca().set_yticklabels([feature_names[i] for i in feature_inds])\n",
        "\n",
        "    elif multi_class and plot_type == \"bar\":\n",
        "        if class_names is None:\n",
        "            class_names = [\"Class \"+str(i) for i in range(len(shap_values))]\n",
        "        feature_inds = feature_order[:max_display]\n",
        "        y_pos = np.arange(len(feature_inds))\n",
        "        left_pos = np.zeros(len(feature_inds))\n",
        "\n",
        "        class_inds = np.argsort([-np.abs(shap_values[i]).mean() for i in range(len(shap_values))])\n",
        "        for i,ind in enumerate(class_inds):\n",
        "            global_shap_values = np.abs(shap_values[ind]).mean(0)\n",
        "            pl.barh(\n",
        "                y_pos, global_shap_values[feature_inds], 0.7, left=left_pos, align='center',\n",
        "                color=colors.default_blue_colors[min(i, len(colors.default_blue_colors)-1)], label=class_names[ind]\n",
        "            )\n",
        "            left_pos += global_shap_values[feature_inds]\n",
        "        pl.yticks(y_pos, fontsize=13)\n",
        "        pl.gca().set_yticklabels([feature_names[i] for i in feature_inds])\n",
        "        pl.legend(frameon=False, fontsize=12)\n",
        "\n",
        "    # draw the color bar\n",
        "    if color_bar and features is not None and plot_type != \"bar\" and \\\n",
        "            (plot_type != \"layered_violin\" or color in pl.cm.datad):\n",
        "        import matplotlib.cm as cm\n",
        "        m = cm.ScalarMappable(cmap=cmap if plot_type != \"layered_violin\" else pl.get_cmap(color))\n",
        "        m.set_array([0, 1])\n",
        "        cb = pl.colorbar(m, ticks=[0, 1], aspect=1000)\n",
        "        cb.set_ticklabels([labels['FEATURE_VALUE_LOW'], labels['FEATURE_VALUE_HIGH']])\n",
        "        cb.set_label(labels['FEATURE_VALUE'], size=12, labelpad=0)\n",
        "        cb.ax.tick_params(labelsize=11, length=0)\n",
        "        cb.set_alpha(1)\n",
        "        cb.outline.set_visible(False)\n",
        "        bbox = cb.ax.get_window_extent().transformed(pl.gcf().dpi_scale_trans.inverted())\n",
        "        cb.ax.set_aspect((bbox.height - 0.9) * 20)\n",
        "        # cb.draw_all()\n",
        "\n",
        "    pl.gca().xaxis.set_ticks_position('bottom')\n",
        "    pl.gca().yaxis.set_ticks_position('none')\n",
        "    pl.gca().spines['right'].set_visible(False)\n",
        "    pl.gca().spines['top'].set_visible(False)\n",
        "    pl.gca().spines['left'].set_visible(False)\n",
        "    pl.gca().tick_params(color=axis_color, labelcolor=axis_color)\n",
        "    pl.yticks(range(len(feature_order)), [feature_names[i] for i in feature_order], fontsize=13)\n",
        "    if plot_type != \"bar\":\n",
        "        pl.gca().tick_params('y', length=20, width=0.5, which='major')\n",
        "    pl.gca().tick_params('x', labelsize=11)\n",
        "    pl.ylim(-1, len(feature_order))\n",
        "    if plot_type == \"bar\":\n",
        "        pl.gca().axes.get_yaxis().set_visible(False)\n",
        "        pl.xlabel(\"Mean |SHAP| value\", fontsize=13)\n",
        "    else:\n",
        "        pl.xlabel(\"SHAP value\", fontsize=13)\n",
        "    if show:\n",
        "        pl.show()\n",
        "    return fig\n",
        "\n",
        "def shorten_text(text, length_limit):\n",
        "    if len(text) > length_limit:\n",
        "        return text[:length_limit - 3] + \"...\"\n",
        "    else:\n",
        "        return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e1Cc3iOLsxp"
      },
      "source": [
        "# Comparison tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veJ3Q5Hc1HuY"
      },
      "source": [
        "## General comparison utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryGbDgnv1Mpr"
      },
      "source": [
        "#@title Functions { form-width: \"150px\" }\n",
        "\n",
        "def to_percent(fraction):\n",
        "    \"\"\"Converts a fraction to a percentage string.\"\"\"\n",
        "    return f\"{100*fraction:.2f}%\"\n",
        "\n",
        "    \n",
        "def get_true_ranking(data):\n",
        "    \"\"\"Produces a pairwise ranking dictionary from the data.\n",
        "\n",
        "    Args:\n",
        "        data: The Data instance to rank.\n",
        "    \n",
        "    Returns:\n",
        "        A dictionary mapping pairs of indices matching to position of two guides\n",
        "        in the dataset, to booleans, where the boolean indicates if the first\n",
        "        guide in the pair is better than the second, according to the labels in\n",
        "        the data. Guides that have the same label are not paired.\n",
        "        \n",
        "        For example:\n",
        "        {\n",
        "            (1, 4): True\n",
        "            (2, 5): False\n",
        "        }\n",
        "        means guide number 1 is better than guide number 4, and that guide\n",
        "        number 5 is better than guide number 2.\n",
        "    \"\"\"\n",
        "    pairs = {}\n",
        "    for i1 in range(len(data.labels)):\n",
        "        for i2 in range(i1+1, len(data.labels)):\n",
        "            label1 = data.labels[i1].item()\n",
        "            label2 = data.labels[i2].item()\n",
        "            if label1 == label2: continue\n",
        "            # A lower label means better efficiency in negative screenings.\n",
        "            is_first_better = label1 < label2\n",
        "            pairs[(i1, i2)] = is_first_better\n",
        "    return pairs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzKjCBp91cHu"
      },
      "source": [
        "#@title Comparison container { form-width: \"150px\" }\n",
        "\n",
        "class Comparison(object):\n",
        "    \"\"\"Holds the information required for making comparisons.\n",
        "\n",
        "    Attributes:\n",
        "        datasets: A dictionary mapping dataset names to Data instances.\n",
        "        threshold: Guides with labels below the threshold are considered to be\n",
        "            efficient, the rest are inefficient.\n",
        "        true_rankings: A dictionary mapping corresponding dataset names to their\n",
        "            true pairwise rankings, in the format described for the return value\n",
        "            of get_true_ranking.\n",
        "        efficients: A dictionary mapping corresponding dataset names to their\n",
        "            list of indices of efficient guides.\n",
        "    \"\"\"\n",
        "    def __init__(self, datasets, threshold=-1):\n",
        "        \"\"\"Initialises a Comparison container.\n",
        "\n",
        "        Args:\n",
        "            datasets: A dictionary mapping dataset names to Data instances.\n",
        "            threshold: Guides with labels below the threshold are considered\n",
        "                to be efficient, the rest are inefficient. (by defulat, the\n",
        "                threshold is -1).\n",
        "        \"\"\"\n",
        "        self.datasets = datasets\n",
        "        self.threshold = threshold\n",
        "\n",
        "        self.true_rankings = {}\n",
        "        self.efficients = {}\n",
        "        for data_name, data in datasets.items():\n",
        "            self.true_rankings[data_name] = get_true_ranking(data)\n",
        "            self.efficients[data_name] = data.get_efficient(threshold)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7sHlD9fRWbZ"
      },
      "source": [
        "## Compare pairwise ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ezFMoCuGG8R"
      },
      "source": [
        "#@title Functions { form-width: \"150px\" }\n",
        "\n",
        "\n",
        "def ranking_from_scores(data, score_col, true_ranking):\n",
        "    \"\"\"Produces the ranking precision of a specific tool.\n",
        "\n",
        "    Args:\n",
        "        data: The Data instance to rank.\n",
        "        score_col: The index of the tool's score within the feature\n",
        "            representaion of the guides.\n",
        "        true_ranking: The true pairwise ranking, in the format described for the\n",
        "            return value of get_true_ranking.\n",
        "    \n",
        "    Returns:\n",
        "        The percentage of pairs the tool got right out of all the pairs in the\n",
        "        true_ranking (as a string).\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "\n",
        "    for pair, is_first_better in true_ranking.items():\n",
        "        i1, i2 = pair\n",
        "        score1 = data.features[i1][score_col]\n",
        "        score2 = data.features[i2][score_col]\n",
        "        if (score1 > score2) == is_first_better:\n",
        "            correct += 1\n",
        "    \n",
        "    return to_percent(float(correct) / len(true_ranking))\n",
        "\n",
        "\n",
        "def ranking_from_predictions(data, model, true_ranking, predictions=None):\n",
        "    \"\"\"Produce the ranking precision of a model.\n",
        "\n",
        "    Args:\n",
        "        data: The Data instance to rank.\n",
        "        model: A model which inhertis from BaseModel.\n",
        "        true_ranking: The true pairwise ranking, in the format described for the\n",
        "            return value of get_true_ranking.\n",
        "        predictions: If provided, these are considered to be the scores\n",
        "            predicted by the model for the guides in the data. Otherwise, the\n",
        "            predictions are produced using the model provided.\n",
        "\n",
        "    Returns:\n",
        "        The percentage of pairs the model got right out of all the pairs in the\n",
        "        true_ranking (as a string).\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    if predictions is None:\n",
        "        predictions = model.get_processed_predictions(data)\n",
        "\n",
        "    for pair, is_first_better in true_ranking.items():\n",
        "        i1, i2 = pair\n",
        "        label1 = predictions[i1].item()\n",
        "        label2 = predictions[i2].item()\n",
        "        # The models are trained to give lower scores to better guides.\n",
        "        predicted_first_better = label1 < label2\n",
        "        if predicted_first_better == is_first_better:\n",
        "            correct += 1\n",
        "    \n",
        "    return to_percent(float(correct) / len(true_ranking))\n",
        "\n",
        "\n",
        "def ranking_from_majority(data, true_ranking):\n",
        "    \"\"\"Produces the ranking precision of the Majority Vote method.\n",
        "\n",
        "    Args:\n",
        "        data: The Data instance to rank.\n",
        "        true_ranking: The true pairwise ranking, in the format described for the\n",
        "            return value of get_true_ranking.\n",
        "\n",
        "    Returns:\n",
        "        The percentage of pairs the Majority Vote got right out of all the pairs\n",
        "        in the true_ranking (as a string).\n",
        "        The Majority Vote is composed of all the scoring and decision tools.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    score_cols = list(COL_TO_SCORE_TOOL.keys())\n",
        "    decision_cols = list(COL_TO_DECISION_TOOL.keys())\n",
        "    decision_cols = list(set(decision_cols) - set(score_cols))\n",
        "\n",
        "    decisions = []\n",
        "    for col in decision_cols:\n",
        "        decision_function = COL_TO_DECISION_FUNCTION[col]\n",
        "        decisions.append(decision_function(data, col))\n",
        "\n",
        "    for pair, is_first_better in true_ranking.items():\n",
        "        i1, i2 = pair\n",
        "        votes1, votes2 = 0, 0\n",
        "        for col in score_cols:\n",
        "            score1 = data.features[i1][col]\n",
        "            score2 = data.features[i2][col]\n",
        "            if score1 >= score2: votes1 += 1\n",
        "            if score2 >= score1: votes2 += 1\n",
        "        for decision in decisions:\n",
        "            score1 = decision[i1]\n",
        "            score2 = decision[i2]\n",
        "            if score1 >= score2: votes1 += 1\n",
        "            if score2 >= score1: votes2 += 1\n",
        "\n",
        "        predicted_first_better =  votes1 > votes2\n",
        "        if predicted_first_better == is_first_better:\n",
        "            correct += 1\n",
        "\n",
        "    return to_percent(float(correct) / len(true_ranking))\n",
        "\n",
        "\n",
        "def ranking_from_deepcrispr(data, true_ranking, results_path):\n",
        "    \"\"\"Produces the ranking precision of DeepCRISPR.\n",
        "\n",
        "    Args:\n",
        "        data: The Data instance to rank.\n",
        "        true_ranking: The true pairwise ranking, in the format described for the\n",
        "            return value of get_true_ranking.\n",
        "        results_path: The path of the DeepCRISPR results file.\n",
        "\n",
        "    Returns:\n",
        "        The percentage of pairs DeepCRISPR got right out of all the pairs in\n",
        "        the true_ranking (as a string).\n",
        "    \"\"\"\n",
        "    with open(results_path, 'rb') as fd:\n",
        "        deepcrispr_cores = pickle.load(fd)\n",
        "\n",
        "    deepcrispr_data = Data(num_features=1)\n",
        "    deepcrispr_data.targets = data.targets\n",
        "    deepcrispr_data.labels = data.labels\n",
        "    for target in data.targets:\n",
        "        score = deepcrispr_cores[target]\n",
        "        deepcrispr_data.features.append([score])\n",
        "\n",
        "    return ranking_from_scores(deepcrispr_data, 0, true_ranking)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwn9duftPO8I"
      },
      "source": [
        "#@title Report { form-width: \"150px\" }\n",
        "\n",
        "def tools_ranking_report(comparison):\n",
        "    \"\"\"Produces the ranking precision report of all the scoring tools.\n",
        "\n",
        "    Args:\n",
        "        comparison: A Comparison instance for the desired datasets.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the precision report.\n",
        "    \"\"\"\n",
        "    rankings = {}\n",
        "    for data_name, data in comparison.datasets.items():\n",
        "        rankings[data_name] = []\n",
        "        for col in COL_TO_SCORE_TOOL.keys():\n",
        "            precision = ranking_from_scores(\n",
        "                data, col, comparison.true_rankings[data_name])\n",
        "            rankings[data_name].append(precision)\n",
        "\n",
        "    return _create_report(rankings, list(COL_TO_SCORE_TOOL.values()))\n",
        "\n",
        "\n",
        "def majority_ranking_report(comparison):\n",
        "    \"\"\"Produces the ranking precision report of the Majority Vote.\n",
        "\n",
        "    Args:\n",
        "        comparison: A Comparison instance for the desired datasets.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the precision report.\n",
        "    \"\"\"\n",
        "    rankings = {}\n",
        "    for data_name, data in comparison.datasets.items():\n",
        "        rankings[data_name] = [\n",
        "            ranking_from_majority(data, comparison.true_rankings[data_name])]\n",
        "\n",
        "    return _create_report(rankings, [\"Majority vote\"])\n",
        "\n",
        "\n",
        "def model_ranking_report(comparison, model, name=\"Model\"):\n",
        "    \"\"\"Produce the ranking precision report of a model.\n",
        "\n",
        "    Args:\n",
        "        comparison: A Comparison instance for the desired datasets.\n",
        "        model: A model which inherits from BaseModel, to produce the report for.\n",
        "        name: The name to give the model in the report (\"Model\" by default).\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the precision report.\n",
        "    \"\"\"\n",
        "    rankings = {}\n",
        "    for data_name, data in comparison.datasets.items():\n",
        "        precision = ranking_from_predictions(\n",
        "            data, model, comparison.true_rankings[data_name])\n",
        "        rankings[data_name] = [precision]\n",
        "\n",
        "    return _create_report(rankings, [name])\n",
        "\n",
        "\n",
        "def get_ranking_report(comparison, model):\n",
        "    \"\"\"Produce the full ranking precision report.\n",
        "\n",
        "    Args:\n",
        "        comparison: A Comparison instance for the desired datasets.\n",
        "        model: A model which inherits from BaseModel, to include in the report.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the precision report for all the scoring tools, the\n",
        "        model and the Majority Vote method.\n",
        "    \"\"\"\n",
        "    tools = tools_ranking_report(comparison)\n",
        "    majority = majority_ranking_report(comparison)\n",
        "    model = model_ranking_report(comparison, model)\n",
        "    return tools.append(majority, ignore_index=True).\\\n",
        "                 append(model, ignore_index=True)\n",
        "\n",
        "\n",
        "def deepcrispr_ranking_report(comparison, variants_dict=VARIANTS):\n",
        "    \"\"\"Produces the ranking precision report of DeepCRISPR.\n",
        "\n",
        "    Args:\n",
        "        comparison: A Comparison instance for the desired datasets.\n",
        "        variants_dict: A dictionary mapping DeepCRISPR versions to their\n",
        "        variants (VARIANTS by default).\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the precision report.\n",
        "    \"\"\"\n",
        "    rankings = {}\n",
        "    models = [f\"DeepCRISPR ({version} {variant})\"\\\n",
        "              for version, variants in variants_dict.items()\\\n",
        "              for variant in variants]\n",
        "    for data_name, data in comparison.datasets.items():\n",
        "        rankings[data_name] = []\n",
        "        for version, variants in variants_dict.items():\n",
        "            for variant in variants:\n",
        "                results_path = get_deepcrispr_path(version, variant, data_name)\n",
        "                precision = ranking_from_deepcrispr(\n",
        "                    data, comparison.true_rankings[data_name], results_path)\n",
        "                rankings[data_name].append(precision)\n",
        "    \n",
        "    return _create_report(rankings, models)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO_I7ZvDTqtC"
      },
      "source": [
        "## Compare binary decisions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7sL9r9cVfyU"
      },
      "source": [
        "#@title Decision functions { form-width: \"150px\" }\n",
        "\n",
        "\"\"\"Utilities for extracting binary decisions from the dataset\"\"\"\n",
        "\n",
        "def _get_scores(data, col, score_type=int):\n",
        "    \"\"\"Extracts the raw scores from the data.\n",
        "\n",
        "    Args:\n",
        "        data: A Data instance to extract from.\n",
        "        col: The index of the score within the feature representaion of a guide.\n",
        "        score_type: The type to cast the score into (int by default).\n",
        "    \n",
        "    Returns:\n",
        "        A list with the score of each guide in the data, with the order\n",
        "        preserved.\n",
        "    \"\"\"\n",
        "    return [score_type(features[col]) for features in data.features]\n",
        "\n",
        "def get_phyto_decisions(data, col):\n",
        "    \"\"\"Translates the PhytoCRISP-Ex scores to 0 (reject) / 1 (accept)\n",
        "    \n",
        "    Args:\n",
        "        data: A Data instance to extract from.\n",
        "        col: The index of the tool's score within the feature representaion of a\n",
        "            guide.\n",
        "    \n",
        "    Returns:\n",
        "        A list of 0s and 1s, where 1s replace the scores of accepted guides and 0s\n",
        "        replace the scored of rejected guides.\n",
        "    \"\"\"\n",
        "    raw_scores = _get_scores(data, col)\n",
        "    return [(score+3)//4 for score in raw_scores]\n",
        "\n",
        "def get_sgrna_decisions(data, col):\n",
        "    \"\"\"Translates the sgRNA Scorer 2.0 scores to 0 (reject) / 1 (accept)\n",
        "    \n",
        "    Args:\n",
        "        data: A Data instance to extract from.\n",
        "        col: The index of the tool's score within the feature representaion of a\n",
        "            guide.\n",
        "    \n",
        "    Returns:\n",
        "        A list of 0s and 1s, where 1s replace the scores of accepted guides and 0s\n",
        "        replace the scored of rejected guides.\n",
        "    \"\"\"\n",
        "    raw_scores =_get_scores(data, col, score_type=float)\n",
        "    return [1 if score > 0  else 0 for score in raw_scores]\n",
        "\n",
        "def get_model_decisions(scores, threshold):\n",
        "    \"\"\"Translates the model scores to 0 (reject) / 1 (accept)\n",
        "    \n",
        "    Args:\n",
        "        scores: A list of the scores given by the model.\n",
        "        threshold: Scores below threshold are considered to belong to accepted\n",
        "            guides, the rest are rejected.\n",
        "    \n",
        "    Returns:\n",
        "        A list of 0s and 1s, where 1s replace the scores of accepted guides and\n",
        "        0s replace the scored of rejected guides.\n",
        "    \"\"\"\n",
        "    return [1 if score < threshold else 0 for score in scores]\n",
        "\n",
        "\n",
        "# A mapping between a tool's column in the feature representaion and a function\n",
        "# which translates its scores to a 0/1 decisions list.\n",
        "COL_TO_DECISION_FUNCTION = {\n",
        "    5: _get_scores,             # CHOPCHOP: G20\n",
        "    14: _get_scores,            # mm10db: accepted\n",
        "    15: get_phyto_decisions,    # phytoCRISP-Ex\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBL9yqCksnPY"
      },
      "source": [
        "#@title Stats functions { form-width: \"150px\" }\n",
        "\n",
        "\n",
        "# A mapping for constructing the output of get_majority_stats, for internal use.\n",
        "_IDX_TO_MAJORITY_TYPE = {\n",
        "    0: \"Any\",\n",
        "    1: \"Majority\",\n",
        "    2: \"Big majority\",\n",
        "}\n",
        "\n",
        "\n",
        "def get_tool_stats(data, decision_col, efficient, stats_func,\n",
        "                   col_decision_map=COL_TO_DECISION_FUNCTION):\n",
        "    \"\"\"Computes some statistic for a tool for a dataset.\n",
        "    \n",
        "    Args:\n",
        "        data: A Data instance to use the tool on.\n",
        "        decision_col: The index of the decision tool's score in the feature\n",
        "            representation of a guide.\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "        stats_func: A function which takes efficient and a list of binary\n",
        "            decisions and returns some statistic.\n",
        "        col_decision_map: A dictionary mapping indices in the feature \n",
        "            representation to their corresponding decision tools\n",
        "            (COL_TO_DECISION_FUNCTION by default).\n",
        "    \n",
        "    Returns:\n",
        "        The desired statistic.\n",
        "    \"\"\"\n",
        "    decision_func = col_decision_map[decision_col]\n",
        "    decisions = decision_func(data, decision_col)\n",
        "    return stats_func(efficient, decisions)\n",
        "\n",
        "\n",
        "def get_model_stats(data, model, efficient, threshold, stats_func,\n",
        "                    predictions=None):\n",
        "    \"\"\"Computes some statistic for a model for a dataset.\n",
        "    \n",
        "    Args:\n",
        "        data: A Data instance to use the tool on.\n",
        "        model: A model which inherits from BaseModel.\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "        threshold: Scores below threshold are considered to belong to accepted\n",
        "            guides, the rest are rejected.\n",
        "        stats_func: A function which takes efficient and a list of binary\n",
        "            decisions and returns some statistic.\n",
        "        predictions: If provided, these are considered to be the scores \n",
        "            predicted by the model for the guides in the data. Otherwise, the\n",
        "            predictions are produced using the model provided.\n",
        "    \n",
        "    Returns:\n",
        "        The desired statistic.\n",
        "    \"\"\"\n",
        "    if predictions is None:\n",
        "        predictions = model.get_processed_predictions(data).flatten()\n",
        "    decisions = get_model_decisions(predictions, threshold)\n",
        "    return stats_func(efficient, decisions)\n",
        "\n",
        "\n",
        "def get_majority_stats(data, efficient, stats_func,\n",
        "                       col_decision_map=COL_TO_DECISION_FUNCTION):\n",
        "    \"\"\"Computes some statistic for decision Majority Vote methods.\n",
        "    \n",
        "    Args:\n",
        "        data: A Data instance to use the Majority Vote on.\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "        stats_func: A function which takes efficient and a list of binary\n",
        "            decisions and returns some statistic.\n",
        "        col_decision_map: A dictionary mapping indices in the feature\n",
        "            representation to their corresponding decision tools\n",
        "            (COL_TO_DECISION_FUNCTION by default).\n",
        "    \n",
        "    Returns:\n",
        "        A dictionary mapping from the name of the Majority Vote method to its\n",
        "        statistic. The following methods are implemented:\n",
        "            Any: Not actually a majority vote. Accepts all guides which were\n",
        "                accepted by at least one decision tool.\n",
        "            Majority: Accepts all guides which were accepted by at least half of \n",
        "                the decision tools.\n",
        "            Big majority: Only included when the number of decision tools\n",
        "                considered is even. Accepts all guides which were accepted by\n",
        "                more than half of the decision tools.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    decision_cols = list(col_decision_map.keys())\n",
        "    even = len(decision_cols) % 2 == 0\n",
        "    half_votes = np.ceil(len(decision_cols) / 2.0)\n",
        "    \n",
        "    # tool_decisions[i][j] will be the decision made by tool number i regarding\n",
        "    # target number j.\n",
        "    tool_decisions = []\n",
        "    for col in decision_cols:\n",
        "        decision_function = col_decision_map[col]\n",
        "        tool_decisions.append(decision_function(data, col))\n",
        "\n",
        "    # Threshold votes for the different Majority Vote methods.\n",
        "    min_votes = [1, half_votes]\n",
        "    # If the number of tools is even, include the Big Majority method.\n",
        "    if even: min_votes.append(half_votes + 1)\n",
        "    vote_decisions = {threshold: [0] * data.num for threshold in min_votes}\n",
        "\n",
        "    for i in range(data.num):\n",
        "        votes = sum([decision[i] for decision in tool_decisions])\n",
        "        for threshold, decisions in vote_decisions.items():\n",
        "            if votes >= threshold: decisions[i] = 1\n",
        "    \n",
        "    stats = [stats_func(efficient, decisions)\\\n",
        "             for decisions in vote_decisions.values()]\n",
        "    return {\n",
        "        _IDX_TO_MAJORITY_TYPE[i]: stats[i] for i in range(len(stats))\n",
        "    }\n",
        "    \n",
        "\n",
        "def get_deepcrispr_stats(data, efficient, results_path, stats_func):\n",
        "    \"\"\"Computes some statistic for DeepCRISPR for a dataset.\n",
        "    \n",
        "    Args:\n",
        "        data: A Data instance to evaluate DeppCRISPR on.\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "        results_path: The path of the DeepCRISPR results file.\n",
        "        stats_func: A function which takes efficient and a list of binary\n",
        "            decisions and returns some statistic.\n",
        "    \n",
        "    Returns:\n",
        "        The desired statistic.\n",
        "    \"\"\"\n",
        "    with open(results_path, 'rb') as fd:\n",
        "        deepcrispr_cores = pickle.load(fd)\n",
        "\n",
        "    decisions = []\n",
        "    for target in data.targets:\n",
        "        score = deepcrispr_cores[target]\n",
        "        decision = 1 if score > 0.5 else 0\n",
        "        decisions.append(decision)\n",
        "    \n",
        "    return stats_func(efficient, decisions)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24VQ11KYkn-b"
      },
      "source": [
        "#@title Precision functions { form-width: \"150px\" }\n",
        "\n",
        "def get_precision(efficient, decisions): \n",
        "    \"\"\"Returns the decision precision.\n",
        "\n",
        "    Args:\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "        decisions: A list of binary decisions for each of the guides.\n",
        "    \n",
        "    Returns:\n",
        "        The percentage of efficient guides out of the accepted guides.\n",
        "    \"\"\"\n",
        "    correct, accepted = 0, 0\n",
        "    for i, decision in enumerate(decisions):\n",
        "        if not decision: continue\n",
        "        accepted += 1\n",
        "        if i in efficient: correct += 1\n",
        "    if not accepted:\n",
        "        return 0\n",
        "    return 100*float(correct)/accepted\n",
        "\n",
        "def get_tool_precision(data, decision_col, efficient):\n",
        "    \"\"\"Returns the decision precision for a tool.\n",
        "\n",
        "    Args:\n",
        "        data: A Data instance to use the tool on.\n",
        "        decision_col: The index of the decision tool's score in the feature\n",
        "            representation of a guide.\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "    \n",
        "    Returns:\n",
        "        The percentage of efficient guides out of the accepted guides.\n",
        "    \"\"\"\n",
        "    return get_tool_stats(data, decision_col, efficient, get_precision)\n",
        "\n",
        "def get_model_precision(data, model, efficient, threshold, predictions=None):\n",
        "    \"\"\"Returns the decision precision for a model.\n",
        "\n",
        "    Args:\n",
        "        data: A Data instance to use the tool on.\n",
        "        model: A model which inherits from BaseModel.\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "        threshold: Scores below threshold are considered to belong to accepted\n",
        "            guides, the rest are rejected.\n",
        "        predictions: If provided, these are considered to be the scores\n",
        "            predicted by the model for the guides in the data. Otherwise, the\n",
        "            predictions are produced using the model provided.\n",
        "    \n",
        "    Returns:\n",
        "        The percentage of efficient guides out of the accepted guides.\n",
        "    \"\"\"\n",
        "    return get_model_stats(\n",
        "        data, model, efficient, threshold, get_precision, predictions)\n",
        "\n",
        "def get_majority_precision(data, efficient,\n",
        "                           col_decision_map=COL_TO_DECISION_FUNCTION):\n",
        "    \"\"\"Returnד the decision precision for the Majority Vote methods.\n",
        "\n",
        "    Args:\n",
        "        data: A Data instance to use the tool on.\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "        col_decision_map: A dictionary mapping indices in the feature\n",
        "            representation to their corresponding decision tools\n",
        "            (COL_TO_DECISION_FUNCTION by default). These are the tools which\n",
        "            will be used to implement the votes.\n",
        "    \n",
        "    Returns:\n",
        "        The percentage of efficient guides out of the accepted guides for each\n",
        "        of the Majority Vote methods, according to the format defined for the\n",
        "        output of get_majority_stats.\n",
        "    \"\"\"\n",
        "    return get_majority_stats(data, efficient, get_precision, col_decision_map)\n",
        "\n",
        "def get_deepcrispr_precision(data, efficient, results_path):\n",
        "    \"\"\"Returns the decision precision for DeepCRISPR as a decision tool.\n",
        "    \n",
        "    Args:\n",
        "        data: A Data instance to use the tool on.\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "        results_path: The path of the DeepCRISPR results file.\n",
        "    \n",
        "    Returns:\n",
        "        The percentage of efficient guides out of the accepted guides.\n",
        "    \"\"\"\n",
        "    return get_deepcrispr_stats(data, efficient, results_path, get_precision)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjeTJRq4Xz_d"
      },
      "source": [
        "#@title Coverage functions { form-width: \"150px\" }\n",
        "\n",
        "def get_coverage(efficient, decisions):\n",
        "    \"\"\"Returns the decision coverage.\n",
        "\n",
        "    Args:\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "        decisions: A list of binary decisions for each of the guides.\n",
        "    \n",
        "    Returns:\n",
        "        The percentage of efficient guides that were accepted.\n",
        "    \"\"\"\n",
        "    covered = 0\n",
        "    \n",
        "    for i in efficient:\n",
        "        if decisions[i]:\n",
        "            covered += 1\n",
        "    return 100*float(covered)/len(efficient)\n",
        "\n",
        "def get_tool_coverage(data, decision_col, efficient):\n",
        "    \"\"\"Returns the decision coverage for a tool.\n",
        "\n",
        "    Args:\n",
        "        data: A Data instance to use the tool on.\n",
        "        decision_col: The index of the decision tool's score in the feature\n",
        "            representation of a guide.\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "    \n",
        "    Returns:\n",
        "        The percentage of efficient guides that were accepted.\n",
        "    \"\"\"\n",
        "    return get_tool_stats(data, decision_col, efficient, get_coverage)\n",
        "\n",
        "def get_model_coverage(data, model, efficient, threshold, predictions=None):\n",
        "    \"\"\"Return the decision coverage for a model.\n",
        "\n",
        "    Args:\n",
        "        data: A Data instance to use the tool on.\n",
        "        model: A model which inherits from BaseModel.\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "        threshold: Scores below threshold are considered to belong to accepted\n",
        "            guides, the rest are rejected.\n",
        "        predictions: If provided, these are considered to be the scores\n",
        "            predicted by the model for the guides in the data. Otherwise, the\n",
        "            predictions are produced using the model provided.\n",
        "    \n",
        "    Returns:\n",
        "        The percentage of efficient guides that were accepted.\n",
        "    \"\"\"\n",
        "    return get_model_stats(\n",
        "        data, model, efficient, threshold, get_coverage, predictions)\n",
        "\n",
        "def get_majority_coverage(data, efficient,\n",
        "                          col_decision_map=COL_TO_DECISION_FUNCTION):\n",
        "    \"\"\"Returns the decision coverage for the Majority Vote methods.\n",
        "\n",
        "    Args:\n",
        "        data: A Data instance to use the Majority Vote on.\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "        col_decision_map: A dictionary mapping indices in the feature\n",
        "            representation to their corresponding decision tools\n",
        "            (COL_TO_DECISION_FUNCTION by default). These are the tools which\n",
        "            will be used to implement the votes.\n",
        "    \n",
        "    Returns:\n",
        "        The percentage of efficient guides that were accepted for each of the\n",
        "        Majority Vote methods, according to the format defined for the output of\n",
        "        get_majority_stats.\n",
        "    \"\"\"\n",
        "    return get_majority_stats(data, efficient, get_coverage, col_decision_map)\n",
        "\n",
        "def get_deepcrispr_coverage(data, efficient, results_path):\n",
        "    \"\"\"Return the decision covreage for DeepCRISPR as a decision tool.\n",
        "    \n",
        "    Args:\n",
        "        data: A Data instance to use the tool on.\n",
        "        efficient: A list of the indices of the guides in the data considered to\n",
        "            be efficient.\n",
        "        results_path: The path of the DeepCRISPR results file.\n",
        "    \n",
        "    Returns:\n",
        "        The percentage of efficient guides that were accepted.\n",
        "    \"\"\"\n",
        "    return get_deepcrispr_stats(data, efficient, results_path, get_coverage)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN6yTj8oYx8W"
      },
      "source": [
        "### Reporting utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c81ijMCRY4-I"
      },
      "source": [
        "#@title Precision and coverage plot { form-width: \"150px\" }\n",
        "\n",
        "def plot_precision_coverage(data, model, num_points, label_cutoff=-1,\n",
        "                            save_name=\"\"):\n",
        "    \"\"\"Plots the precision and coverage as a function of threshold.\n",
        "\n",
        "    Args:\n",
        "        data: A Data instance to use the model on.\n",
        "        model: A model which inherits from BaseModel.\n",
        "        num_points: Number of (evenly spaced) thresholds to use.\n",
        "        label_cutoff: The label below which guides are considered efficient.\n",
        "        save_name: If provided, the plot is saved under this name in WORK_DIR.\n",
        "    \n",
        "    Returns:\n",
        "        A list of the thresholds used.\n",
        "    \"\"\"\n",
        "    scores = model.get_processed_predictions(data).flatten().double()\n",
        "    efficient = data.get_efficient(label_cutoff)\n",
        "\n",
        "    low_bound = min(scores)\n",
        "    up_bound = max(scores)\n",
        "    interval = (up_bound-low_bound)/(num_points-1)\n",
        "    \n",
        "    thresholds = [low_bound + n*interval for n in range(num_points+1)]\n",
        "    precisions = [get_model_precision(data, model, efficient, t, scores)\\\n",
        "                  for t in thresholds]\n",
        "    coverages = [get_model_coverage(data, model, efficient, t, scores)\\\n",
        "                 for t in thresholds]\n",
        "\n",
        "    prep_plot()\n",
        "    fig = plt.figure()\n",
        "\n",
        "    plt.plot(thresholds, precisions, label='Precision', marker='o', alpha=0.5)\n",
        "    plt.plot(thresholds, coverages, label='Coverage', marker='o', alpha=0.5)\n",
        "\n",
        "    plt.axhline(50, linestyle='--', color='gray')\n",
        "    plt.legend(loc='best')\n",
        "    plt.xlabel(\"Threshold\")\n",
        "    plt.show()\n",
        "\n",
        "    if save_name:\n",
        "        fig.savefig(os.path.join(WORK_DIR, save_name),\n",
        "                    bbox_inches=\"tight\", dpi=100)\n",
        "    return thresholds\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrAhfBdeOiQg"
      },
      "source": [
        "#@title Precision and coverage report { form-width: \"150px\" }\n",
        "\n",
        "STATS_STR = \"precision: %.02f\\ncoverage:  %.02f\"\n",
        "\n",
        "def tool_decision_report(comparison):\n",
        "    \"\"\"Produces a precision/coverage report for the decision tools.\n",
        "\n",
        "    Args:\n",
        "        comparison: A Comparison instance for the desired datasets.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the decision precision and coverage report.\n",
        "    \"\"\"\n",
        "    stats = {}\n",
        "\n",
        "    for data_name, data in comparison.datasets.items():\n",
        "        stats[data_name] = []\n",
        "        for col in COL_TO_DECISION_TOOL.keys():\n",
        "            precision = get_tool_precision(\n",
        "                data, col, comparison.efficients[data_name])\n",
        "            coverage = get_tool_coverage(\n",
        "                data, col, comparison.efficients[data_name])\n",
        "            stats[data_name].append(STATS_STR % (precision, coverage))\n",
        "\n",
        "    return _create_report(stats, list(COL_TO_DECISION_TOOL.values()))\n",
        "\n",
        "def majority_decision_report(comparison,\n",
        "                             col_decision_map=COL_TO_DECISION_FUNCTION):\n",
        "    \"\"\"Produces a precision/coverage report for the Majority Vote methods.\n",
        "\n",
        "    Args:\n",
        "        comparison: A Comparison instance for the desired datasets.\n",
        "        col_decision_map: A dictionary mapping indices in the feature \n",
        "            representation to their corresponding decision tools\n",
        "            (COL_TO_DECISION_FUNCTION by default). These are the tools which\n",
        "            will be used to implement the votes.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the decision precision and coverage report.\n",
        "    \"\"\"\n",
        "    stats = {}\n",
        "    vote_tools = []\n",
        "    for data_name, data in comparison.datasets.items():\n",
        "        precisions = get_majority_precision(\n",
        "            data, comparison.efficients[data_name], col_decision_map)\n",
        "        coverages = get_majority_coverage(\n",
        "            data, comparison.efficients[data_name], col_decision_map)\n",
        "\n",
        "        if not vote_tools:\n",
        "            vote_tools = [f\"{maj_type} vote\" for maj_type in precisions.keys()]\n",
        "\n",
        "        stats[data_name] = [\n",
        "            STATS_STR % (precisions[name], coverages[name])\\\n",
        "            for name in precisions.keys()\n",
        "        ]\n",
        "\n",
        "    return _create_report(stats, vote_tools)\n",
        "\n",
        "def model_decision_report(comparison, model, threshold, name=\"Model\"):\n",
        "    \"\"\"Produce a precision/coverage report for a model.\n",
        "\n",
        "    Args:\n",
        "        comparison: A Comparison instance for the desired datasets.\n",
        "        model: A model which inherits from BaseModel.\n",
        "        threshold: Scores below threshold are considered to belong to accepted\n",
        "            guides, the rest are rejected.\n",
        "        name: The name to give the model in the report (\"Model\" by default).\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the decision precision and coverage report.\n",
        "    \"\"\"\n",
        "    stats = {}\n",
        "    for data_name, data in comparison.datasets.items():\n",
        "        precision = get_model_precision(\n",
        "            data, model, comparison.efficients[data_name], threshold)\n",
        "        coverage = get_model_coverage(\n",
        "            data, model, comparison.efficients[data_name], threshold)\n",
        "        stats[data_name] = [STATS_STR % (precision, coverage)]\n",
        "\n",
        "    return _create_report(stats, [name])\n",
        "\n",
        "def get_decision_report(comparison, model, threshold):\n",
        "    \"\"\"Produce a complete precision/coverage report.\n",
        "\n",
        "    Args:\n",
        "        comparison: A Comparison instance for the desired datasets.\n",
        "        model: A model which inherits from BaseModel.\n",
        "        threshold: Model scores below threshold are considered to belong to\n",
        "            accepted guides, the rest are rejected.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the decision precision and coverage report.\n",
        "    \"\"\"\n",
        "    tools = tool_decision_report(comparison)\n",
        "    majority = majority_decision_report(comparison)\n",
        "    model = model_decision_report(comparison, model, threshold)\n",
        "    return tools.append(majority, ignore_index=True).\\\n",
        "                 append(model, ignore_index=True)\n",
        "\n",
        "\n",
        "def deepcrispr_decision_report(comparison, variants_dict):\n",
        "    \"\"\"Produceד a precision/coverage report for DeepCRISPR as a decision tool.\n",
        "    \n",
        "    Args:\n",
        "        comparison: A Comparison instance for the desired datasets.\n",
        "        variants_dict: A dictionary mapping DeepCRISPR versions to its variants\n",
        "        (VARIANTS by default).\n",
        "    \n",
        "    Returns:\n",
        "        The percentage of efficient guides that were accepted.\n",
        "    \"\"\"\n",
        "    stats = {}\n",
        "\n",
        "    for data_name, data in comparison.datasets.items():\n",
        "        stats[data_name] = []\n",
        "        for variant in variants_dict:\n",
        "            results_path = get_deepcrispr_path(\n",
        "                CLASSIFICATION, variant, data_name)\n",
        "            efficient = comparison.efficients[data_name]\n",
        "            precision = get_deepcrispr_precision(data, efficient, results_path)\n",
        "            coverage = get_deepcrispr_coverage(data, efficient, results_path)\n",
        "            stats[data_name].append(STATS_STR % (precision, coverage))\n",
        "\n",
        "    variants_names = [f\"DeepCRISPR ({variant})\" for variant in variants_dict]\n",
        "    return _create_report(stats, variants_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMK2g2texGJ6"
      },
      "source": [
        "## Compare ROC-AUC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmbTG9fGxINE"
      },
      "source": [
        "#@title Functions { form-width: \"150px\" }\n",
        "\n",
        "def get_model_rocauc(data, model, binary_labels):\n",
        "    \"\"\"Returns the ROC-AUC of a model.\n",
        "\n",
        "    Args:\n",
        "        data: A Data instance to evaluate the model on.\n",
        "        model: A model which inherits from BaseModel.\n",
        "        binary_labels: A list of 0s and 1s, where a 1 at index i indicates that\n",
        "            guide i in the data is efficient, and a 0 indicates it is\n",
        "            inefficient.\n",
        "    \n",
        "    Returns:\n",
        "        The ROC-AUC.\n",
        "    \"\"\"\n",
        "    predictions = model.get_processed_predictions(data)\n",
        "    # The model's scores are inverted (lower means better)\n",
        "    predictions = [-p for p in predictions]\n",
        "    return metrics.roc_auc_score(binary_labels, predictions)\n",
        "\n",
        "def get_deepcrispr_rocauc(data, results_path, binary_labels):\n",
        "    \"\"\"Returns the ROC-AUC of DeepCRISPR as a decision tool.\n",
        "\n",
        "    Args:\n",
        "        data: A Data instance to use the tool on.\n",
        "        results_path: The path of the DeepCRISPR results file.\n",
        "        binary_labels: A list of 0s and 1s, where a 1 at index i indicates that\n",
        "            guide i in the data is efficient, and a 0 indicates it is\n",
        "            inefficient.\n",
        "    \n",
        "    Returns:\n",
        "        The ROC-AUC.\n",
        "    \"\"\"\n",
        "    with open(results_path, 'rb') as fd:\n",
        "        results = pickle.load(fd)\n",
        "\n",
        "    scores = []\n",
        "    for target in data.targets:\n",
        "        scores.append(results[target])\n",
        "    \n",
        "    return metrics.roc_auc_score(binary_labels, scores)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2h50jQQxYmv"
      },
      "source": [
        "#@title Report { form-width: \"150px\" }\n",
        "\n",
        "\n",
        "def compare_auc(comparison, model):\n",
        "    \"\"\"Produces a ROC-AUC report.\n",
        "\n",
        "    Args:\n",
        "        comparison: A Comparison instance for the desired datasets.\n",
        "        model: A model which inherits from BaseModel.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame with the ROC-AUC report.\n",
        "    \"\"\"\n",
        "    aucs = {}\n",
        "    for data_name, data in comparison.datasets.items():\n",
        "        efficient = comparison.efficients[data_name]\n",
        "        binary_labels = [1 if i in efficient else 0\\\n",
        "                         for i in range(data.num)]\n",
        "        \n",
        "        model_rocauc = get_model_rocauc(data, model, binary_labels)\n",
        "        results_path = get_deepcrispr_path(REGRESSIONS, \"seq_only\", data_name)\n",
        "        deepcrispr_rocauc = get_deepcrispr_rocauc(\n",
        "            data, results_path, binary_labels)\n",
        "        \n",
        "        aucs[data_name] = [model_rocauc, deepcrispr_rocauc]\n",
        "    \n",
        "    return _create_report(aucs, [\"Model\", \"DeepCRISPR\"], \"%.2f\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-lwdbyPne3m"
      },
      "source": [
        "# Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vizk7a9Ygq8f"
      },
      "source": [
        "#@title Loss functions { form-width: \"150px\" }\n",
        "\n",
        "\"\"\"Adapted from the allRank library by Przemek Pobrotyn.\n",
        "https://github.com/allegro/allRank\n",
        "\n",
        "Main changes: the addition of stochastic rankNet, which does not consider all\n",
        "    the possible pairs, rather it samples a limited number of datapoints twice,\n",
        "    and considers all the possible pairs between them, thus avoiding filling up\n",
        "    the RAM with a huge list of pairs. The size of the sample is controlled by\n",
        "    the new argument sample_size. The wrapper function stochasticRankNet takes\n",
        "    advantage of this argument.\n",
        "\"\"\"\n",
        "\n",
        "DEFAULT_EPS = 1e-10\n",
        "PADDED_Y_VALUE = -100\n",
        "SAMPLE_SIZE = int(6000)\n",
        "\n",
        "\n",
        "def rankNet(y_pred, y_true, padded_value_indicator=PADDED_Y_VALUE, weight_by_diff=False, weight_by_diff_powed=False, sample_size=0):\n",
        "    \"\"\"\n",
        "    RankNet loss introduced in \"Learning to Rank using Gradient Descent\".\n",
        "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
        "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
        "    :param weight_by_diff: flag indicating whether to weight the score differences by ground truth differences.\n",
        "    :param weight_by_diff_powed: flag indicating whether to weight the score differences by the squared ground truth differences.\n",
        "    :return: loss value, a torch.Tensor\n",
        "    \"\"\"\n",
        "    y_pred = y_pred.clone().unsqueeze(0).squeeze(2)\n",
        "    y_true = y_true.clone().unsqueeze(0).squeeze(2)\n",
        "\n",
        "    mask = y_true == padded_value_indicator\n",
        "    y_pred[mask] = float('-inf')\n",
        "    y_true[mask] = float('-inf')\n",
        "\n",
        "    if sample_size and sample_size < y_true.shape[1]:\n",
        "        # sample sample_size twice, and generate every pair of indices from the samples\n",
        "        sample1 = random.sample(range(y_true.shape[1]), sample_size)\n",
        "        sample2 = random.sample(range(y_true.shape[1]), sample_size)\n",
        "        document_pairs_candidates = list(product(sample1, sample2))\n",
        "    else:\n",
        "        # here we generate every pair of indices from the range of document length in the batch\n",
        "        document_pairs_candidates = list(product(range(y_true.shape[1]), repeat=2))\n",
        "\n",
        "    document_pairs_candidates = list(product(range(y_true.shape[1]), repeat=2))\n",
        "\n",
        "    pairs_true = y_true[:, document_pairs_candidates]\n",
        "    selected_pred = y_pred[:, document_pairs_candidates]\n",
        "\n",
        "    # here we calculate the relative true relevance of every candidate pair\n",
        "    true_diffs = pairs_true[:, :, 0] - pairs_true[:, :, 1]\n",
        "    pred_diffs = selected_pred[:, :, 0] - selected_pred[:, :, 1]\n",
        "    \n",
        "\n",
        "    # here we filter just the pairs that are 'positive' and did not involve a padded instance\n",
        "    # we can do that since in the candidate pairs we had symetric pairs so we can stick with\n",
        "    # positive ones for a simpler loss function formulation\n",
        "    the_mask = (true_diffs > 0) & (~torch.isinf(true_diffs))\n",
        "\n",
        "    pred_diffs = pred_diffs[the_mask]\n",
        "\n",
        "    weight = None\n",
        "    if weight_by_diff:\n",
        "        abs_diff = torch.abs(true_diffs)\n",
        "        weight = abs_diff[the_mask]\n",
        "    elif weight_by_diff_powed:\n",
        "        true_pow_diffs = torch.pow(pairs_true[:, :, 0], 2) - torch.pow(pairs_true[:, :, 1], 2)\n",
        "        abs_diff = torch.abs(true_pow_diffs)\n",
        "        weight = abs_diff[the_mask]\n",
        "\n",
        "    # here we 'binarize' true relevancy diffs since for a pairwise loss we just need to know\n",
        "    # whether one document is better than the other and not about the actual difference in\n",
        "    # their relevancy levels\n",
        "    true_diffs = (true_diffs > 0).type(torch.float32)\n",
        "    true_diffs = true_diffs[the_mask]\n",
        "   \n",
        "    return BCEWithLogitsLoss(weight=weight)(pred_diffs, true_diffs)\n",
        "\n",
        "\n",
        "def stochasticRankNet(y_pred, y_true, padded_value_indicator=PADDED_Y_VALUE,\n",
        "                      weight_by_diff=False, weight_by_diff_powed=False):\n",
        "    return RankNet(y_pred, y_true, padded_value_indicator, weight_by_diff,\n",
        "                   weight_by_diff_powed, SAMPLE_SIZE)\n",
        "\n",
        "\n",
        "def lambdaLoss(y_pred, y_true, eps=DEFAULT_EPS, padded_value_indicator=PADDED_Y_VALUE, weighing_scheme=None, k=None, sigma=1., mu=10.,\n",
        "               reduction=\"mean\", reduction_log=\"binary\"):\n",
        "    \"\"\"\n",
        "    LambdaLoss framework for LTR losses implementations, introduced in \"The LambdaLoss Framework for Ranking Metric Optimization\".\n",
        "    Contains implementations of different weighing schemes corresponding to e.g. LambdaRank or RankNet.\n",
        "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
        "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
        "    :param eps: epsilon value, used for numerical stability\n",
        "    :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
        "    :param weighing_scheme: a string corresponding to a name of one of the weighing schemes\n",
        "    :param k: rank at which the loss is truncated\n",
        "    :param sigma: score difference weight used in the sigmoid function\n",
        "    :param mu: optional weight used in NDCGLoss2++ weighing scheme\n",
        "    :param reduction: losses reduction method, could be either a sum or a mean\n",
        "    :param reduction_log: logarithm variant used prior to masking and loss reduction, either binary or natural\n",
        "    :return: loss value, a torch.Tensor\n",
        "    \"\"\"\n",
        "    device = y_pred.device\n",
        "    y_pred = y_pred.clone().unsqueeze(0).squeeze(2)\n",
        "    y_true = y_true.clone().unsqueeze(0).squeeze(2)\n",
        "\n",
        "    padded_mask = y_true == padded_value_indicator\n",
        "    y_pred[padded_mask] = float(\"-inf\")\n",
        "    y_true[padded_mask] = float(\"-inf\")\n",
        "\n",
        "    # Here we sort the true and predicted relevancy scores.\n",
        "    y_pred_sorted, indices_pred = y_pred.sort(descending=True, dim=-1)\n",
        "    y_true_sorted, _ = y_true.sort(descending=True, dim=-1)\n",
        "\n",
        "    # After sorting, we can mask out the pairs of indices (i, j) containing index of a padded element.\n",
        "    true_sorted_by_preds = torch.gather(y_true, dim=1, index=indices_pred)\n",
        "    true_diffs = true_sorted_by_preds[:, :, None] - true_sorted_by_preds[:, None, :]\n",
        "    padded_pairs_mask = torch.isfinite(true_diffs)\n",
        "\n",
        "    if weighing_scheme != \"ndcgLoss1_scheme\":\n",
        "        padded_pairs_mask = padded_pairs_mask & (true_diffs > 0)\n",
        "\n",
        "    ndcg_at_k_mask = torch.zeros((y_pred.shape[1], y_pred.shape[1]), dtype=torch.bool, device=device)\n",
        "    ndcg_at_k_mask[:k, :k] = 1\n",
        "\n",
        "    # Here we clamp the -infs to get correct gains and ideal DCGs (maxDCGs)\n",
        "    true_sorted_by_preds.clamp_(min=0.)\n",
        "    y_true_sorted.clamp_(min=0.)\n",
        "\n",
        "    # Here we find the gains, discounts and ideal DCGs per slate.\n",
        "    pos_idxs = torch.arange(1, y_pred.shape[1] + 1).to(device)\n",
        "    D = torch.log2(1. + pos_idxs.float())[None, :]\n",
        "    maxDCGs = torch.sum(((torch.pow(2, y_true_sorted) - 1) / D)[:, :k], dim=-1).clamp(min=eps)\n",
        "    G = (torch.pow(2, true_sorted_by_preds) - 1) / maxDCGs[:, None]\n",
        "\n",
        "    # Here we apply appropriate weighing scheme - ndcgLoss1, ndcgLoss2, ndcgLoss2++ or no weights (=1.0)\n",
        "    if weighing_scheme is None:\n",
        "        weights = 1.\n",
        "    else:\n",
        "        weights = globals()[weighing_scheme](G, D, mu, true_sorted_by_preds)  # type: ignore\n",
        "\n",
        "    # We are clamping the array entries to maintain correct backprop (log(0) and division by 0)\n",
        "    scores_diffs = (y_pred_sorted[:, :, None] - y_pred_sorted[:, None, :]).clamp(min=-1e8, max=1e8)\n",
        "    scores_diffs[torch.isnan(scores_diffs)] = 0.\n",
        "    weighted_probas = (torch.sigmoid(sigma * scores_diffs).clamp(min=eps) ** weights).clamp(min=eps)\n",
        "    if reduction_log == \"natural\":\n",
        "        losses = torch.log(weighted_probas)\n",
        "    elif reduction_log == \"binary\":\n",
        "        losses = torch.log2(weighted_probas)\n",
        "    else:\n",
        "        raise ValueError(\"Reduction logarithm base can be either natural or binary\")\n",
        "\n",
        "    masked_losses = losses[padded_pairs_mask & ndcg_at_k_mask]\n",
        "    if reduction == \"sum\":\n",
        "        loss = -torch.sum(masked_losses)\n",
        "    elif reduction == \"mean\":\n",
        "        loss = -torch.mean(masked_losses)\n",
        "    else:\n",
        "        raise ValueError(\"Reduction method can be either sum or mean\")\n",
        "\n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biWAy7M3hGa_"
      },
      "source": [
        "#@title Data class { form-width: \"150px\" }\n",
        "\n",
        "class Data(object):\n",
        "    \"\"\"Represents a dataset.\n",
        "\n",
        "    Attributes:\n",
        "        targets: A list of the target sequences in the dataset.\n",
        "        features: A list of feature representations of the targets (with\n",
        "            matching order).\n",
        "        labels: A list of the labels of the targets (with matching order).\n",
        "        num: The number of targets in the dataset.\n",
        "        num_features: The length of the feature representations.\n",
        "        epoch_counter: The number of training epochs performed (for tracking\n",
        "            purposes).\n",
        "        epochs: A list of epochs after which the loss for this dataset was\n",
        "            sampled.\n",
        "        losses: A list of losses measured at the epochs in the apochs list.\n",
        "    \"\"\"\n",
        "    def __init__(self, datapoints=None, num_features=0,\n",
        "                 labels_extractor=get_labels):\n",
        "        \"\"\"Initialises a dataset.\n",
        "\n",
        "        Args:\n",
        "            datapoints: A list of DataPoint instances.\n",
        "            num_features: The length of the feature representations of the\n",
        "                targets.\n",
        "            labels_extractor: A function which extracts labels from the\n",
        "                datapoints.\n",
        "        \"\"\"\n",
        "        if datapoints:\n",
        "            self.targets = get_targets(datapoints)\n",
        "            self.features = get_features(datapoints, num_features)\n",
        "            self.labels = labels_extractor(datapoints)\n",
        "        else:\n",
        "            self.targets = []\n",
        "            self.features = []\n",
        "            self.labels = []\n",
        "        \n",
        "        self.num = len(self.labels)\n",
        "        self.num_features = num_features\n",
        "        self.epoch_counter = 0\n",
        "        self.epochs = []\n",
        "        self.losses = []\n",
        "    \n",
        "    def shuffle(self):\n",
        "        \"\"\"Returns a shuffled copy of the dataset.\"\"\"\n",
        "        permutation = torch.randperm(self.num)\n",
        "        shuffled = Data(num_features=self.num_features)\n",
        "        shuffled.features = self.features[permutation]\n",
        "        shuffled.labels = self.labels[permutation]\n",
        "        shuffled.num = self.num\n",
        "        return shuffled\n",
        "    \n",
        "    def get_efficient(self, threshold=-1):\n",
        "        \"\"\"Returns a list of the indices of the efficient guides in the dataset.\n",
        "\n",
        "        Args:\n",
        "            threshold: Guides with a label below threshold are considered\n",
        "                efficient.\n",
        "        \n",
        "        Returns:\n",
        "            The list of indices of efficient guides.\n",
        "        \"\"\"\n",
        "        efficient = []\n",
        "        for i,label in enumerate(self.labels):\n",
        "            if label < threshold:\n",
        "                efficient.append(i)\n",
        "        return efficient\n",
        "    \n",
        "    def update_progress(self, epoch_counter, loss):\n",
        "        \"\"\"Updates the training progress of the dataset.\n",
        "        \n",
        "        Args:\n",
        "            epoch_counter: The number of elapsed epochs.\n",
        "            loss: The latest loss registered for the dataset.\n",
        "        \"\"\"\n",
        "        self.epoch_counter = epoch_counter\n",
        "        self.epochs.append(epoch_counter)\n",
        "        self.losses.append(loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih8lZX6R17K-"
      },
      "source": [
        "#@title BaseModel { form-width: \"150px\" }\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "    \"\"\"A basic model all model should inherit from\n",
        "\n",
        "    Attributes:\n",
        "        input_size: The dimension of the input, should correspond to the size of\n",
        "            the feature representation of a guide.\n",
        "        output_size: The number of outputs.\n",
        "        hidden_size: The number of units in a hidden layer.\n",
        "        epoch_counter: The number of epochs the model has been trained for.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size=0):\n",
        "        super(BaseModel, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.epoch_counter = 0\n",
        "    \n",
        "    def get_name(self):\n",
        "        \"\"\"Returns the name of the model (which is the class name)\"\"\"\n",
        "        return self.__class__.__name__\n",
        "\n",
        "    def get_predictions(self, data, is_train=True):\n",
        "        \"\"\"Returns the raw predictions of the model for the data.\n",
        "        \n",
        "        Args:\n",
        "            data: A Data instance.\n",
        "            is_train: A boolean indicating if the model is being trained on the\n",
        "                data. If True, gradients will be computed.\n",
        "        \n",
        "        Returns:\n",
        "            The predictions of the model for the data.\n",
        "        \"\"\"\n",
        "        with torch.set_grad_enabled(is_train):\n",
        "            x_features = Variable(torch.from_numpy(data.features)).to(device)\n",
        "            return self(x_features)\n",
        "    \n",
        "    def get_processed_predictions(self, data):\n",
        "        \"\"\"Returns the post-processed predictions of the model for the data.\n",
        "        \n",
        "        Args:\n",
        "            data: A Data instance.\n",
        "        \n",
        "        Returns:\n",
        "            The processedpredictions of the model for the data.\n",
        "        \"\"\"\n",
        "        # In this basic model, no post-processing is needed, so jsut return the\n",
        "        # predictions.\n",
        "        return self.get_predictions(data, False)\n",
        "\n",
        "    def count_params(self):\n",
        "        \"\"\"Returns the number of learnt parameters of the model.\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nBlucjXeUMD"
      },
      "source": [
        "#@title Experiment class { form-width: \"150px\" }\n",
        "\n",
        "class Experiment(object):\n",
        "    \"\"\"A class to manage an experiment.\n",
        "    \n",
        "    Attributes:\n",
        "        model: The experimental model (which inherits from BaseModel).\n",
        "        optimizer: A PyTorch Optimizer (initialised).\n",
        "        criterion: A loss function.\n",
        "        num_features: The size of the feature representaion.\n",
        "        labels_extractor: A method for extracting labels from datapoints.\n",
        "        lr: The learning rate for the optimizer.\n",
        "        name: A name representing the experiment.\n",
        "        training: A Data instance for the training set.\n",
        "        validation: A Data instance for the validation set.\n",
        "        testing: A Data instance for the test set.\n",
        "        tracked_datasets: A dictionary mapping dataset names to Data instances\n",
        "            for these datasets. These will be updated with the model's loss for\n",
        "            them throughout the training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, criterion,\n",
        "                 num_features, labels_extractor=get_labels,\n",
        "                 lr=0.01, weight_decay=0.0001, suffix=\"\"):\n",
        "        \"\"\"Initialise an experiment.\n",
        "\n",
        "        Args:\n",
        "            model: The experimentatl model (which inherits from BaseModel).\n",
        "            optimizer: A PyTorch Optimizer (uninitialised).\n",
        "            criterion: A loss function.\n",
        "            num_features: The size of the feature representaion.\n",
        "            labels_extractor: A method for extracting labels from datapoints.\n",
        "            lr: The learning rate for the optimizer.\n",
        "            weight_decay: The L2 regularisation parameter.\n",
        "            suffix: A suffix for the name of the experiment.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer(self.model.parameters(), lr=lr,\n",
        "                                   weight_decay=weight_decay)\n",
        "        self.criterion = criterion\n",
        "        self.num_features = num_features\n",
        "        self.labels_extractor = labels_extractor\n",
        "        self.lr = lr\n",
        "        \n",
        "        date_str = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
        "        self.name = \"%s_%s\" % (model.get_name(), date_str)\n",
        "        if suffix:\n",
        "            self.name += f\"_{suffix}\"\n",
        "\n",
        "        print(\"Starting experiment with model %s with %s params\" %\n",
        "              (self.model.get_name(), self.model.count_params()))\n",
        "        \n",
        "        self.training = None\n",
        "        self.validation = None\n",
        "        self.testing = None\n",
        "        self.tracked_datasets = {}\n",
        "        \n",
        "    def set_data(self, train_data, validation_data, test_data):\n",
        "        \"\"\"Initialises basic datasets for the experiment.\n",
        "\n",
        "        Args:\n",
        "            train_data: A list of DataPoint instances for training.\n",
        "            validation_data: A list of DataPoint instances for validation.\n",
        "            test_data: A list of DataPoint instances for testing.\n",
        "        \"\"\"\n",
        "        self.training = Data(\n",
        "            train_data, self.num_features, self.labels_extractor)\n",
        "        self.validation = Data(\n",
        "            validation_data, self.num_features, self.labels_extractor)\n",
        "        self.testing = Data(\n",
        "            test_data, self.num_features, self.labels_extractor)\n",
        "\n",
        "        self.tracked_datasets[\"validation\"] = self.validation\n",
        "        self.tracked_datasets[\"test\"] = self.testing\n",
        "    \n",
        "    def add_dataset(self, raw_data, name):\n",
        "        \"\"\"Adds a dataset to the tracked datasets.\n",
        "        \n",
        "        Args:\n",
        "            raw_data: The list of DataPoint instances of the dataset.\n",
        "            name: A name to represent the dataset.\n",
        "        \"\"\"\n",
        "        self.tracked_datasets[name] = Data(\n",
        "            raw_data, self.num_features, self.labels_extractor)\n",
        "\n",
        "    def prep_and_set_data(self, datasets, genome_data, sizes):\n",
        "        \"\"\"Initialises all the required datasets.\n",
        "\n",
        "        Args:\n",
        "            datasets: A dictionary mapping dataset names to their list of\n",
        "                DataPoint instances. These will be used when composing the work\n",
        "                data (that is, the training, validation and test sets) along\n",
        "                with the genome data. They will also be added to the tracked\n",
        "                datasets.\n",
        "            genome_data: A list of DataPoint instances which will be sampled\n",
        "                from to complete the work data.\n",
        "            sizes: A tuple with the sizes of the three standard datasets:\n",
        "                training, validation and test (in that order).\n",
        "        \"\"\"\n",
        "        # Concatenates the datasets\n",
        "        work_data = sum([data for data in datasets.values()], [])\n",
        "        training_size, validation_size, test_size = sizes\n",
        "        total_size = sum(sizes)\n",
        "        \n",
        "        genome_padding_len = total_size - len(work_data)\n",
        "        genome_padding = random.sample(genome_data, genome_padding_len)\n",
        "        work_data += genome_padding\n",
        "        np.random.shuffle(work_data)\n",
        "\n",
        "        work_datasets = []\n",
        "        start = 0\n",
        "        for i in range(len(sizes)):\n",
        "            end = start + sizes[i]\n",
        "            work_datasets.append(work_data[start:end])\n",
        "            start = end\n",
        "    \n",
        "        self.set_data(*work_datasets)\n",
        "        for name, dataset in datasets.items():\n",
        "            self.add_dataset(dataset, name)\n",
        "\n",
        "    def set_lr(self, lr):\n",
        "        \"\"\"Changes the learning rate.\"\"\"\n",
        "        self.lr = lr\n",
        "        for g in self.optimizer.param_groups:\n",
        "            g['lr'] = lr\n",
        "\n",
        "    def train(self, epochs=int(1e5), report_every=0):\n",
        "        \"\"\"Trains the model.\n",
        "\n",
        "        The general structure of this training procedure was adapted from the\n",
        "        PyTorch tutorial:\n",
        "        NLP From Scratch: Translation with a Sequence to Sequence Network and\n",
        "        Attention.\n",
        "        https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\n",
        "        Args:\n",
        "            epochs: Number of epochs to train for.\n",
        "            report_every: The number of epochs after which to report progress\n",
        "                and update the datasets with the current loss.\n",
        "        \"\"\"\n",
        "        if not report_every:\n",
        "            report_every = epochs // 10\n",
        "        start_time = time.time()\n",
        "        data = self.training.shuffle()\n",
        "        print_loss = 0\n",
        "\n",
        "        for epoch in range(1, epochs+1):\n",
        "            self.model.train()\n",
        "            self.model.epoch_counter += 1\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            y = Variable(torch.from_numpy(data.labels)).to(device)\n",
        "            outputs = self.model.get_predictions(data, True)\n",
        "            loss = self.criterion(outputs, y)\n",
        "            print_loss += loss\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if epoch % report_every == 0:\n",
        "                report_progress(self, start_time, epoch, epochs,\n",
        "                                print_loss, loss, report_every)\n",
        "                print_loss = 0\n",
        "    \n",
        "    def test(self):\n",
        "        \"\"\"Tests the model and updates progress for tracked datasets.\n",
        "\n",
        "        Returns:\n",
        "            A list with two items (in this order): the current loos for the\n",
        "            validation set and the current loss for the test set.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        for data in self.tracked_datasets.values():\n",
        "            y = Variable(torch.from_numpy(data.labels)).to(device)\n",
        "            outputs = self.model.get_predictions(data, False)\n",
        "            loss = self.criterion(outputs, y)\n",
        "            data.update_progress(self.model.epoch_counter, loss)\n",
        "\n",
        "        losses = [\n",
        "            self.tracked_datasets[\"validation\"].losses[-1],\n",
        "            self.tracked_datasets[\"test\"].losses[-1]\n",
        "        ]\n",
        "        return losses\n",
        "    \n",
        "    def get_experiment_dir(self):\n",
        "        \"\"\"Returns the path of the experiment's directory.\"\"\"\n",
        "        return self._get_experiment_dir(self.name)\n",
        "    \n",
        "    def get_model_params_path(self):\n",
        "        \"\"\"Returns the path of the model's state file.\"\"\"\n",
        "        return self._get_model_params_path(self.name)\n",
        "    \n",
        "    def get_experiment_path(self):\n",
        "        \"\"\"Returns the path of the experiment's Pickle file.\"\"\"\n",
        "        return self._get_experiment_path(self.name)\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\"Saves the experiment and the model.\"\"\"\n",
        "        if not os.path.exists(self.get_experiment_dir()):\n",
        "            os.makedirs(self.get_experiment_dir())\n",
        "\n",
        "        torch.save(self.model.state_dict(), self.get_model_params_path())\n",
        "        with open(self.get_experiment_path(), 'wb') as fd:\n",
        "            pickle.dump(self, fd)\n",
        "\n",
        "    @classmethod\n",
        "    def _get_experiment_dir(cls, name):\n",
        "        \"\"\"Returns the path of the experiment's directory with this name.\"\"\"\n",
        "        return os.path.join(WORK_DIR, \"models\", name)\n",
        "    \n",
        "    @classmethod\n",
        "    def _get_model_params_path(cls, name):\n",
        "        \"\"\"Returns the path of the model's state given the experiment name.\"\"\"\n",
        "        return os.path.join(cls._get_experiment_dir(name), \"model_params\")\n",
        "\n",
        "    @classmethod\n",
        "    def _get_experiment_path(cls, name):\n",
        "        \"\"\"Returns the path of the experiment's  Pickle file.\"\"\"\n",
        "        return os.path.join(cls._get_experiment_dir(name), \"experiment.pkl\")\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, name):\n",
        "        \"\"\"Loads the experiment with the given name.\"\"\"\n",
        "        path = cls._get_experiment_path(name)\n",
        "        with open(path, 'rb') as fd:\n",
        "            experiment = pickle.load(fd)\n",
        "        experiment.name = name\n",
        "        experiment.model.load_state_dict(\n",
        "            torch.load(cls._get_model_params_path(name), map_location=device))\n",
        "        experiment.model = experiment.model.to(device)\n",
        "        return experiment\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stv-S3F6Gxy3"
      },
      "source": [
        "#@title Fully connnected experiemnt { form-width: \"150px\" }\n",
        "\n",
        "class FullyConnected(BaseModel):\n",
        "    \"\"\"A fully connected feedforward neural network.\n",
        "    \n",
        "    Attributes (changes from BaseModel):\n",
        "        num_layers: The number of neural layers (this includes the hidden layers\n",
        "            and the output layer, but not the input layer).\n",
        "        nn: A PyTorch Sequential model, which holds all the layers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
        "        \"\"\"Initialises the model\n",
        "        \n",
        "        Args:\n",
        "            input_size: The dimension of the input, should correspond to the\n",
        "                size of the feature representation of a guide.\n",
        "            output_size: The number of outputs.\n",
        "            hidden_size: The number of units in a hidden layer.\n",
        "            num_layers: The number of neural layers (this includes the hidden\n",
        "                layers and the output layer, but not the input layer).\n",
        "        \"\"\"\n",
        "        super(FullyConnected, self).__init__(\n",
        "            input_size, output_size, hidden_size)\n",
        "        self.num_layers = num_layers\n",
        "        self.nn = nn.Sequential(*self.get_layers())\n",
        "    \n",
        "    def get_layers(self):\n",
        "        \"\"\"Initialises the layers of the network.\"\"\"\n",
        "        if self.num_layers == 1:\n",
        "            return [nn.Linear(self.input_size, self.output_size)]\n",
        "        \n",
        "        layers = [\n",
        "            nn.Linear(self.input_size, self.hidden_size),\n",
        "            nn.Tanh(),\n",
        "        ]\n",
        "        for _ in range(self.num_layers - 2):\n",
        "            layers.append(nn.Linear(self.hidden_size, self.hidden_size))\n",
        "            layers.append(nn.Tanh())\n",
        "        layers.append(nn.Linear(self.hidden_size, self.output_size))\n",
        "        return layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.nn(x)\n",
        "\n",
        "\n",
        "class FullyConnectedExperiment(Experiment):\n",
        "    \"\"\"An experiment with a fully connected feedforward neural network.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_features, hidden_size, num_layers, optimizer,\n",
        "                 criterion, lr=0.01, weight_decay=0.0001, suffix=\"\"):\n",
        "        \"\"\"Initialises the experiment.\n",
        "        \n",
        "        Args:\n",
        "            num_features: The size of the feature representation of guides.\n",
        "            hidden_size: The number of units in a hidden layer.\n",
        "            num_layers: The number of neural layers (this includes the hidden\n",
        "                layers and the output layer, but not the input layer).\n",
        "            optimizer: A PyTorch Optimizer (uninitialised).\n",
        "            criterion: A loss function.\n",
        "            lr: The learning rate for the optimizer.\n",
        "            weight_decay: The L2 regularisation parameter.\n",
        "            suffix: A suffix for the name of the experiment.\n",
        "        \"\"\"\n",
        "        model = FullyConnected(\n",
        "            input_size=num_features,\n",
        "            output_size=1,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "        super(FullyConnectedExperiment, self).__init__(\n",
        "            model, optimizer, criterion, num_features, get_labels,\n",
        "            lr, weight_decay, suffix\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RXszaYreppR"
      },
      "source": [
        "#@title Ordinal classification experiemnt { form-width: \"150px\" }\n",
        "\n",
        "class OrdinalClassifier(BaseModel):\n",
        "    \"\"\"An ordinal classification model.\n",
        "\n",
        "    Follows the architecture proposed by:\n",
        "    Frank, E. and Hall, M. (2001, September)\n",
        "    A simple approach to ordinal classification.\n",
        "    In European Conference on Machine Learning (pp. 145-156). Springer, Berlin,\n",
        "    Heidelberg.\n",
        "    https://link.springer.com/chapter/10.1007/3-540-44795-4_13\n",
        "\n",
        "    Attributes (changes from BaseModel):\n",
        "        num_labels: The number of different labels or classes.\n",
        "        num_layers: The number of neural layers (this includes the hidden\n",
        "            layers and the output layer, but not the input layer) in the\n",
        "            classifiers.\n",
        "        classifiers: A PyTorch ModuleList of the classifiers.\n",
        "        datasets: The copies of the training dataset, after it has been\n",
        "            transformed to a classification dataset for each of the classifiers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, num_labels, hidden_size, num_layers):\n",
        "        \"\"\"Initialises the model.\n",
        "        \n",
        "        Args:\n",
        "            input_size: The dimension of the input, should correspond to the\n",
        "                size of the feature representation of a guide.\n",
        "            num_labels: The number of different labels or classes.\n",
        "            hidden_size: The number of units in a hidden layer.\n",
        "            num_layers: The number of neural layers (this includes the hidden\n",
        "                layers and the output layer, but not the input layer) in the\n",
        "                classifiers.\n",
        "        \"\"\"\n",
        "        super(OrdinalClassifier, self).__init__(\n",
        "            input_size, num_labels, hidden_size)\n",
        "        self.num_labels = num_labels\n",
        "        self.num_layers = num_layers\n",
        "        self.classifiers = nn.ModuleList([\n",
        "            FullyConnected(input_size, 1, hidden_size, num_layers)\\\n",
        "            for _ in range(num_labels-1)\n",
        "        ])\n",
        "        self.datasets = []\n",
        "        \n",
        "    def forward(self, x, label):\n",
        "        \"\"\"Produces predictions for x using the classifier of the label.\"\"\"\n",
        "        # self.classifiers[label] is the classifier for > label\n",
        "        return self.classifiers[label](x)\n",
        "    \n",
        "    def classify(self, data, choose=False):\n",
        "        \"\"\"Produces predictions for the data.\n",
        "\n",
        "        Args:\n",
        "            data: A Data instance to classify.\n",
        "            choose: If False, produces probability distributions over all the\n",
        "                possible classes. If True, chooses the most probable class for\n",
        "                each item in the dataset.\n",
        "        \n",
        "        Returns:\n",
        "            Either a list with the probability distributions or a single\n",
        "            predicted class, depending on the value of choose.\n",
        "        \"\"\"\n",
        "        x = Variable(torch.from_numpy(data.features)).to(device)\n",
        "\n",
        "        # prob_more_than[i][j] is the probability that datapoint j has a label\n",
        "        # higher than i.\n",
        "        prob_more_than = [0] * (self.num_labels - 1)\n",
        "        with torch.no_grad():\n",
        "            for label, classifier in enumerate(self.classifiers):\n",
        "                prob_more_than[label] = torch.sigmoid(classifier(x))\n",
        "        \n",
        "        # prob_equals[i][j] is the probability that datapoint i has label j.\n",
        "        prob_equals = [[0]*self.num_labels for _ in range(data.num)]\n",
        "        for i, probs in enumerate(prob_equals):\n",
        "            probs[0] = 1 - prob_more_than[0][i]\n",
        "            top_label = self.num_labels - 1\n",
        "            for label in range(1, top_label):\n",
        "                probs[label] = prob_more_than[label-1][i] -\\\n",
        "                    prob_more_than[label][i]\n",
        "            probs[top_label] = prob_more_than[top_label - 1][i]\n",
        "        if choose:\n",
        "            classes = [np.argmax(probs) for probs in prob_equals]\n",
        "            return torch.tensor(classes).unsqueeze(1)\n",
        "        \n",
        "        return torch.tensor(prob_equals)\n",
        "    \n",
        "    def split_data(self, data):\n",
        "        \"\"\"Splits the dataset into the required classification datasets.\"\"\"\n",
        "        datasets = [copy.deepcopy(data) for _ in range(self.num_labels - 1)]\n",
        "        for min_label, dataset in enumerate(datasets):\n",
        "            for i, label in enumerate(dataset.labels):\n",
        "                if label > min_label:\n",
        "                    dataset.labels[i] = 1\n",
        "                else:\n",
        "                    dataset.labels[i] = 0\n",
        "        return datasets\n",
        "    \n",
        "    def get_processed_predictions(self, data):\n",
        "        \"\"\"Returns the predicted classes for the data.\"\"\"\n",
        "        return self.classify(data, choose=True)\n",
        "    \n",
        "\n",
        "class OrdinalClassificationExperiment(Experiment):\n",
        "    \"\"\"An ordinal classification experiment.\n",
        "\n",
        "    Attributes (changes from Experiment):\n",
        "        optimizers: PyTorch Optimizers (initialised) for all the classifiers.\n",
        "        weight_decay: The L2 regularisation parameter.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_features, num_labels, hidden_size, num_layers,\n",
        "                 optimizer, criterion, lr=0.01, weight_decay=0.0001, suffix=\"\"):\n",
        "        \"\"\"Initialises the experiment.\n",
        "        \n",
        "        Args:\n",
        "            num_features: The size of the feature representation of guides.\n",
        "            num_labels: The number of different labels or classes.\n",
        "            hidden_size: The number of units in a hidden layer.\n",
        "            num_layers: The number of neural layers (this includes the hidden\n",
        "                layers and the output layer, but not the input layer) in the\n",
        "                classifiers.\n",
        "            optimizer: A PyTorch Optimizer (uninitialised).\n",
        "            criterion: A loss function.\n",
        "            lr: The learning rate for the optimizer.\n",
        "            weight_decay: The L2 regularisation parameter.\n",
        "            suffix: A suffix for the name of the experiment.\n",
        "        \"\"\"\n",
        "        model = OrdinalClassifier(\n",
        "            input_size=num_features,\n",
        "            num_labels=num_labels,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "        super(OrdinalClassificationExperiment, self).__init__(\n",
        "            model, optimizer, criterion, num_features,\n",
        "            get_labels_as_indices, lr, weight_decay, suffix\n",
        "        )\n",
        "        self.weight_decay = weight_decay\n",
        "        self.optimizers = [\n",
        "            self.optimizer(\n",
        "                classifier.parameters(), lr=self.lr, weight_decay=0.0001)\\\n",
        "            for classifier in self.model.classifiers\n",
        "        ]\n",
        "        \n",
        "    def train(self, epochs=int(1e5), report_every=0):\n",
        "        \"\"\"Trains the model.\n",
        "\n",
        "        Args:\n",
        "            epochs: Number of epochs to train for.\n",
        "            report_every: The number of epochs after which to report progress\n",
        "                and update the datasets with the current loss.\n",
        "        \"\"\"\n",
        "        if not report_every:\n",
        "            report_every = epochs // 10\n",
        "        start_time = time.time()\n",
        "        print_loss = 0\n",
        "\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "        datasets = experiment.model.split_data(experiment.training.shuffle())\n",
        "\n",
        "        for epoch in range(1, epochs+1):\n",
        "            experiment.model.train()\n",
        "            experiment.model.epoch_counter += 1\n",
        "            for label, classifier in enumerate(experiment.model.classifiers):\n",
        "                optimizer = self.optimizers[label]\n",
        "                optimizer.zero_grad()\n",
        "                data = datasets[label]\n",
        "\n",
        "                y = Variable(torch.from_numpy(data.labels).float()).\\\n",
        "                    to(device).unsqueeze(1)\n",
        "                outputs = classifier.get_predictions(data, True)\n",
        "                loss = criterion(outputs, y)\n",
        "                print_loss += loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            if epoch % report_every == 0:\n",
        "                report_progress(experiment, start_time, epoch, epochs,\n",
        "                                print_loss, loss, report_every)\n",
        "                print_loss = 0\n",
        "    \n",
        "    def test(self):\n",
        "        \"\"\"Tests the model and update progress for tracked datasets.\n",
        "\n",
        "        Returns:\n",
        "            A list with two items (in this order): the current loos for the\n",
        "            validation set and the current loss for the test set.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for data in self.tracked_datasets.values():\n",
        "            y = Variable(torch.from_numpy(data.labels)).to(device)\n",
        "            outputs = self.model.classify(data, choose=False)\n",
        "            loss = criterion(outputs, y)\n",
        "            data.update_progress(self.model.epoch_counter, loss)\n",
        "        \n",
        "        losses = [\n",
        "            self.tracked_datasets[\"validation\"].losses[-1],\n",
        "            self.tracked_datasets[\"test\"].losses[-1]\n",
        "        ]\n",
        "        return losses\n",
        "    \n",
        "    def set_lr(self, lr):\n",
        "        self.lr = lr\n",
        "        for optimizer in self.optimizers:\n",
        "            for g in optimizer.param_groups:\n",
        "                g['lr'] = lr\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLL2MOV5e0NM"
      },
      "source": [
        "#@title Ordinal regression experiemnt { form-width: \"150px\" }\n",
        "\n",
        "class OrdinalRegression(BaseModel):\n",
        "    \"\"\"An ordinal regression model.\n",
        "\n",
        "    Follows and utilises the architecture proposed by:\n",
        "    Rosenthal, E. (2018)\n",
        "    spacecutter: Ordinal Regression Models in PyTorch.\n",
        "    https://www.ethanrosenthal.com/2018/12/06/spacecutter-ordinal-regression/\n",
        "\n",
        "    Attributes (changes from BaseModel):\n",
        "        ascension: Responsible for clipping the cutpoints to preserve ascenfing\n",
        "            order.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, num_labels, hidden_size, num_layers):\n",
        "        \"\"\"Initialises the model.\n",
        "        \n",
        "        Args:\n",
        "            input_size: The dimension of the input, should correspond to the\n",
        "                size of the feature representation of a guide.\n",
        "            num_labels: The number of different labels or classes.\n",
        "            hidden_size: The number of units in a hidden layer.\n",
        "            num_layers: The number of neural layers (this includes the hidden\n",
        "                layers and the output layer, but not the input layer).\n",
        "        \"\"\"\n",
        "        super(OrdinalRegression, self).__init__(\n",
        "            input_size, num_labels, hidden_size)\n",
        "        self.num_labels = num_labels\n",
        "        self.num_layers = num_layers\n",
        "        self.regression = OrdinalLogisticModel(\n",
        "            FullyConnected(input_size, 1, hidden_size, num_layers),\n",
        "            num_labels\n",
        "        )\n",
        "        self.ascension = AscensionCallback()\n",
        "    \n",
        "    def get_processed_predictions(self, data):\n",
        "        \"\"\"Returns the predicted classes for the data.\"\"\"\n",
        "        outputs = self.get_predictions(data, False)\n",
        "        classes = torch.argmax(outputs, dim=1)\n",
        "        return classes\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.regression(x)\n",
        "\n",
        "\n",
        "class OrdinalRegressionExperiment(Experiment):\n",
        "    \"\"\"An ordinal regression experiment.\"\"\"\n",
        "\n",
        "    def __init__(self, num_features, num_labels, hidden_size, num_layers,\n",
        "                 optimizer, criterion, lr=0.01, weight_decay=0.0001, suffix=\"\"):\n",
        "        \"\"\"Initialises the experiment.\n",
        "        \n",
        "        Args:\n",
        "            num_features: The size of the feature representation of guides.\n",
        "            num_labels: The number of different labels or classes.\n",
        "            hidden_size: The number of units in a hidden layer.\n",
        "            num_layers: The number of neural layers (this includes the hidden\n",
        "                layers and the output layer, but not the input layer).\n",
        "            optimizer: A PyTorch Optimizer (uninitialised).\n",
        "            criterion: A loss function.\n",
        "            lr: The learning rate for the optimizer.\n",
        "            weight_decay: The L2 regularisation parameter.\n",
        "            suffix: A suffix for the name of the experiment.\n",
        "        \"\"\"\n",
        "        model = OrdinalRegression(\n",
        "            input_size=num_features,\n",
        "            num_labels=num_labels,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "        super(OrdinalRegressionExperiment, self).__init__(\n",
        "            model, optimizer, criterion, num_features, get_labels_as_indices,\n",
        "            lr, weight_decay, suffix\n",
        "        )\n",
        "    \n",
        "    def train(self, epochs=int(1e5), report_every=0):\n",
        "        \"\"\"Trains the model.\n",
        "\n",
        "        Args:\n",
        "            epochs: Number of epochs to train for.\n",
        "            report_every: The number of epochs after which to report progress\n",
        "                and update the datasets with the current loss.\n",
        "        \"\"\"\n",
        "        if not report_every:\n",
        "            report_every = epochs // 10\n",
        "        start_time = time.time()\n",
        "        data = self.training.shuffle()\n",
        "        print_loss = 0\n",
        "\n",
        "        for epoch in range(1, epochs+1):\n",
        "            self.model.train()\n",
        "            self.model.epoch_counter += 1\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            y = Variable(torch.from_numpy(data.labels)).\\\n",
        "                to(device).reshape(-1, 1)\n",
        "            outputs = self.model.get_predictions(data, True)\n",
        "\n",
        "            loss = self.criterion(outputs, y)\n",
        "            print_loss += loss\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.model.ascension.clip(self.criterion)\n",
        "\n",
        "            if epoch % report_every == 0:\n",
        "                report_progress(self, start_time, epoch, epochs,\n",
        "                                print_loss, loss, report_every)\n",
        "                print_loss = 0\n",
        "    \n",
        "    def test(self):\n",
        "        \"\"\"Tests the model and update progress for tracked datasets.\n",
        "\n",
        "        Returns:\n",
        "            A list with two items (in this order): the current loos for the\n",
        "            validation set and the current loss for the test set.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        for name, data in self.tracked_datasets.items():\n",
        "            y = Variable(torch.from_numpy(data.labels)).\\\n",
        "                to(device).reshape(-1, 1)\n",
        "            outputs = self.model.get_predictions(data)\n",
        "            loss = self.criterion(outputs, y)\n",
        "            data.epoch_counter = self.model.epoch_counter\n",
        "            data.epochs.append(data.epoch_counter)\n",
        "            data.losses.append(loss)\n",
        "\n",
        "        losses = [\n",
        "            self.tracked_datasets[\"validation\"].losses[-1],\n",
        "            self.tracked_datasets[\"test\"].losses[-1]\n",
        "        ]\n",
        "        return losses\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R5dDwgEyJdY"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBBF45L2yMBu"
      },
      "source": [
        "#@title Get data { form-width: \"150px\" }\n",
        "\n",
        "chari_data = get_data(CHARI_DATA_PATH)\n",
        "genome_data = get_data(GENOME_DATA_PATH)\n",
        "xu_data = get_data(XU_DATA_PATH)\n",
        "nr_xu_data = get_data(NR_XU_DATA_PATH)\n",
        "doench_data = get_data(DOENCH_DATA_PATH)\n",
        "\n",
        "num_features = NUM_FEATURES\n",
        "num_labels = NUM_FEATURES\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI1_lFOx7uVN"
      },
      "source": [
        "## Initialise experiment\n",
        "\n",
        "Following are examples of how to initialised each of the three available types of an experiment.\n",
        "Choose one of these three, or load an existing model instead.\n",
        "\n",
        "1. Fully connected experiment:\n",
        "\n",
        "        experiment = FullyConnectedExperiment(\n",
        "            num_features=num_features,\n",
        "            hidden_size=13,\n",
        "            num_layers=4,\n",
        "            optimizer=torch.optim.SGD,\n",
        "            criterion=rankNet,\n",
        "            lr=0.1,\n",
        "            weight_decay=0.0001,\n",
        "            suffix=\"description\")\n",
        "\n",
        "2. Ordinal classification experiment:\n",
        "\n",
        "        experiment = OrdinalClassificationExperiment(\n",
        "            num_features=num_features,\n",
        "            num_labels=num_labels,\n",
        "            hidden_size=10,\n",
        "            num_layers=4,\n",
        "            optimizer=torch.optim.SGD,\n",
        "            criterion=nn.BCEWithLogitsLoss(),\n",
        "            lr=0.01,\n",
        "            weight_decay=0.0001,\n",
        "            suffix=\"description\")\n",
        "\n",
        "3. Ordinal regression experiment:\n",
        "\n",
        "        experiment = OrdinalRegressionExperiment(\n",
        "            num_features=num_features,\n",
        "            num_labels=num_labels,\n",
        "            hidden_size=15,\n",
        "            num_layers=3,\n",
        "            optimizer=torch.optim.SGD,\n",
        "            criterion=CumulativeLinkLoss(),\n",
        "            lr=0.1,\n",
        "            weight_decay=0.0001,\n",
        "            suffix=\"description\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSjHlgLFpPgl"
      },
      "source": [
        "#@title New experiment { form-width: \"150px\" }\n",
        "\n",
        "experiment = FullyConnectedExperiment(\n",
        "     num_features=num_features,\n",
        "     hidden_size=18,\n",
        "     num_layers=4,\n",
        "     optimizer=torch.optim.SGD,\n",
        "     criterion=lambdaLoss,\n",
        "     lr=0.1,\n",
        "     weight_decay=0.0001,\n",
        "     suffix=\"description\")\n",
        "datasets = {\n",
        "    \"xu\": xu_data,\n",
        "    \"doench\": doench_data,\n",
        "    \"chari\": chari_data,\n",
        "}\n",
        "experiment.prep_and_set_data(datasets, genome_data, SIZES_6K)\n",
        "print(experiment.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOuzwA4k712q"
      },
      "source": [
        "#@title Load experiment { form-width: \"150px\" }\n",
        "\n",
        "experiment = FullyConnectedExperiment.load(\"experiment_name\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOmrUujTHMVv"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7oFjf5Qx8P8"
      },
      "source": [
        "for _ in range(1):\n",
        "    experiment.train(100)\n",
        "    experiment.save()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TqiPSHE8_64"
      },
      "source": [
        "## Evaluate progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a1OyPwz3mA6"
      },
      "source": [
        "plot_standard_losses(experiment, train=False, test=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChZxlkrIqvAy"
      },
      "source": [
        "to_plot = [\"xu\", \"validation\"]\n",
        "plot_losses(\n",
        "    {key.capitalize(): experiment.tracked_datasets[key] for key in to_plot},\n",
        "    ylabel=\"RankNet Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XoJJYYrZwH5"
      },
      "source": [
        "# Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0TAfKu4vfgE"
      },
      "source": [
        "#@title Initialise comparisons { form-width: \"150px\" }\n",
        "\n",
        "with open(MIXTURE_DATA_PATH, 'rb') as fd:\n",
        "    mixture = pickle.load(fd)\n",
        "\n",
        "standard_cmp = Comparison(experiment.tracked_datasets)\n",
        "final_cmp = Comparison({\n",
        "    \"mixture\": Data(mixture, num_features),\n",
        "    \"nr_xu\": Data(nr_xu_data, num_features),\n",
        "    \"xu\": Data(xu_data, num_features),\n",
        "    \"doench\": Data(doench_data, num_features),\n",
        "})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVLL2mBMt0Om"
      },
      "source": [
        "## Against scoring tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh_NtMqoH-9T"
      },
      "source": [
        "#@title Short summary { form-width: \"150px\" }\n",
        "\n",
        "for data_name, data in final_cmp.datasets.items():\n",
        "    true_ranking = final_cmp.true_rankings[data_name]\n",
        "    success_rate = ranking_from_predictions(data, experiment.model, true_ranking)\n",
        "    print(f\"\\t{data_name.ljust(18)} {success_rate}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMguRRhyPTMH"
      },
      "source": [
        "#@title Full report { form-width: \"150px\" }\n",
        "\n",
        "pretty_print(get_ranking_report(final_cmp, experiment.model))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9-BEmeGt2wV"
      },
      "source": [
        "## Against decision tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrWyXmAQt_ku"
      },
      "source": [
        "#@title Precision-coverage plot { form-width: \"150px\" }\n",
        "\n",
        "thresholds = plot_precision_coverage(\n",
        "    experiment.validation, experiment.model, 100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz6OkGntvVwN"
      },
      "source": [
        "#@title Full report { form-width: \"150px\" }\n",
        "\n",
        "df = get_decision_report(final_cmp, experiment.model, 0.12)\n",
        "pretty_print(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JVGib3oH6FD"
      },
      "source": [
        "## Against similar tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6W2vlKdkEAo"
      },
      "source": [
        "#@title Crackling configuration { form-width: \"150px\" }\n",
        "\n",
        "CRACKLING_TOOLS = {\n",
        "    5: COL_TO_DECISION_FUNCTION[5],\n",
        "    14: COL_TO_DECISION_FUNCTION[14],\n",
        "    16: get_sgrna_decisions,\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_f8Ww4oq_1R"
      },
      "source": [
        "#@title Ranking comparison { form-width: \"150px\" }\n",
        "\n",
        "pretty_print(deepcrispr_ranking_report(final_cmp))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ6KMUibzWPe"
      },
      "source": [
        "#@title Decision comparison { form-width: \"150px\" }\n",
        "\n",
        "deepcrispr_df = deepcrispr_decision_report(\n",
        "    final_cmp, VARIANTS[CLASSIFICATION])\n",
        "crackling_df = majority_decision_report(\n",
        "    final_cmp, CRACKLING_TOOLS).\\\n",
        "    drop(0).replace(\"Majority vote\", \"Crackling\")\n",
        "model_df = model_decision_report(final_cmp, experiment.model, 0.12)\n",
        "\n",
        "pretty_print(\n",
        "    deepcrispr_df.\\\n",
        "    append(crackling_df, ignore_index=True).\\\n",
        "    append(model_df, ignore_index=True)\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uxC6zfjy0n0"
      },
      "source": [
        "#@title ROC-AUC comparison { form-width: \"150px\" }\n",
        "\n",
        "pretty_print(compare_auc(final_cmp, experiment.model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVx8uCBGIaSu"
      },
      "source": [
        "# Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsdiJWtMIfi3"
      },
      "source": [
        "#@title Prepare explainer { form-width: \"150px\" }\n",
        "\n",
        "SAMPLE_SIZE = 1000\n",
        "\n",
        "x_train = experiment.training.features\n",
        "\n",
        "e = shap.DeepExplainer(\n",
        "    experiment.model, \n",
        "    torch.from_numpy(\n",
        "        x_train[np.random.choice(\n",
        "            np.arange(len(x_train)), SAMPLE_SIZE, replace=False)]\n",
        "    ).to(device))\n",
        "\n",
        "\n",
        "x_samples = x_train[np.random.choice(\n",
        "    np.arange(len(x_train)), SAMPLE_SIZE, replace=False)]\n",
        "\n",
        "\n",
        "shap_values = e.shap_values(\n",
        "    torch.from_numpy(x_samples).to(device)\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arn2cbZBK3me"
      },
      "source": [
        "#@title Plot dot { form-width: \"150px\" }\n",
        "\n",
        "shapfig_dot = summary_plot(\n",
        "    shap_values,\n",
        "    features=x_samples, feature_names=FEATURES,\n",
        "    axis_color=\"black\", cmap=SHAP_CMAP)\n",
        "\n",
        "shapfig_dot.savefig(os.path.join(WORK_DIR, \"shap_dot.png\"), bbox_inches=\"tight\", dpi=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QkCYMQbK7Dw"
      },
      "source": [
        "#@title Plot bar { form-width: \"150px\" }\n",
        "\n",
        "shapfig_bar = summary_plot(\n",
        "    shap_values,\n",
        "    features=x_samples, feature_names=FEATURES,\n",
        "    plot_type=\"bar\",\n",
        "    axis_color=\"black\", color=SHAP_COLOUR)\n",
        "\n",
        "shapfig_bar.savefig(os.path.join(WORK_DIR, \"shap_bar.png\"), bbox_inches=\"tight\", dpi=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3UifMS63VrX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}